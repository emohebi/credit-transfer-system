{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c55696-47e8-480d-8100-d3b78740813f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./EnvPrep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f8bba4-60e2-4241-8337-c6fd697c9e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef583e7e-6d2c-4018-8f53-2932007875ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration settings for the Credit Transfer Analysis System\n",
    "Updated to include batch processing options\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path(\".\") / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Loaded environment variables from {env_path}\")\n",
    "    else:\n",
    "        print(\".env file is not found!!! .. using Config.py\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load .env file: {e}\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"System configuration with batch processing support\"\"\"\n",
    "    \n",
    "    # Project paths\n",
    "    BASE_DIR = Path(\".\")\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "    CACHE_DIR = BASE_DIR / \"cache\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for dir_path in [DATA_DIR, OUTPUT_DIR, CACHE_DIR]:\n",
    "        dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # GPU Configuration\n",
    "    VLLM_GPU_ID = int(os.getenv(\"VLLM_GPU_ID\", \"0\"))  # GPU ID for vLLM\n",
    "    EMBEDDING_GPU_ID = int(os.getenv(\"EMBEDDING_GPU_ID\", \"1\"))  # GPU ID for embeddings\n",
    "    GPU_MEMORY_UTILIZATION = float(os.getenv(\"GPU_MEMORY_UTILIZATION\", \"0.9\"))  # Memory fraction for vLLM\n",
    "    \n",
    "    # Azure OpenAI Configuration\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"ENDPOINT_URL\", \"https://ehsaninstance1.openai.azure.com/\")\n",
    "    AZURE_OPENAI_DEPLOYMENT = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", None)\n",
    "    AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\")\n",
    "    AZURE_OPENAI_TIMEOUT = int(os.getenv(\"AZURE_OPENAI_TIMEOUT\", \"60\"))\n",
    "    AZURE_OPENAI_MAX_TOKENS = int(os.getenv(\"AZURE_OPENAI_MAX_TOKENS\", \"4000\"))\n",
    "    AZURE_OPENAI_TEMPERATURE = float(os.getenv(\"AZURE_OPENAI_TEMPERATURE\", \"0.0\"))\n",
    "    USE_AZURE_OPENAI = os.getenv(\"USE_AZURE_OPENAI\", \"false\").lower() == \"true\"\n",
    "    \n",
    "    # vLLM Configuration\n",
    "    USE_VLLM = os.getenv(\"USE_VLLM\", \"true\").lower() == \"true\"\n",
    "    VLLM_MODEL_NAME = os.getenv(\"VLLM_MODEL_NAME\", \"gpt-oss-120b\")\n",
    "    VLLM_NUM_GPUS = int(os.getenv(\"VLLM_NUM_GPUS\", \"1\"))\n",
    "    VLLM_MAX_MODEL_LEN = int(os.getenv(\"VLLM_MAX_MODEL_LEN\", \"8192\"))\n",
    "    \n",
    "    # Batch Processing Configuration\n",
    "    USE_VLLM_BATCH = os.getenv(\"USE_VLLM_BATCH\", \"true\").lower() == \"true\"\n",
    "    VLLM_BATCH_SIZE = int(os.getenv(\"VLLM_BATCH_SIZE\", \"8\"))\n",
    "    VLLM_MAX_BATCH_SIZE = int(os.getenv(\"VLLM_MAX_BATCH_SIZE\", \"16\"))\n",
    "    VLLM_BATCH_TIMEOUT = int(os.getenv(\"VLLM_BATCH_TIMEOUT\", \"120\"))\n",
    "    \n",
    "    # Model directories\n",
    "    MODEL_CACHE_DIR = os.getenv(\"MODEL_CACHE_DIR\", \"/root/.cache/huggingface/hub\")\n",
    "    EXTERNAL_MODEL_DIR = os.getenv(\"EXTERNAL_MODEL_DIR\", \"/Volumes/jsa_external_prod/external_vols/scratch/Scratch/Ehsan/Models\")\n",
    "    \n",
    "    # Legacy Web API Configuration (kept for compatibility)\n",
    "    GENAI_ENDPOINT = os.getenv(\"GENAI_ENDPOINT\", \"http://localhost:8080\")\n",
    "    GENAI_API_KEY = os.getenv(\"GENAI_API_KEY\", None)\n",
    "    GENAI_TIMEOUT = int(os.getenv(\"GENAI_TIMEOUT\", \"30\"))\n",
    "    USE_GENAI = os.getenv(\"USE_GENAI\", \"false\").lower() == \"true\"\n",
    "    \n",
    "    # Embedding Configuration\n",
    "    EMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\", \"jinaai--jina-embeddings-v4\")\n",
    "    EMBEDDING_DEVICE = os.getenv(\"EMBEDDING_DEVICE\", \"cuda:1\")\n",
    "    EMBEDDING_BATCH_SIZE = int(os.getenv(\"EMBEDDING_BATCH_SIZE\", \"64\"))\n",
    "    EMBEDDING_MODE = os.getenv(\"EMBEDDING_MODE\", \"embedding\")\n",
    "    \n",
    "    # Legacy embedding configurations\n",
    "    EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    EMBEDDING_ENDPOINT = os.getenv(\"EMBEDDING_ENDPOINT\", None)\n",
    "    EMBEDDING_API_KEY = os.getenv(\"EMBEDDING_API_KEY\", None)\n",
    "    USE_EMBEDDING_API = os.getenv(\"USE_EMBEDDING_API\", \"false\").lower() == \"true\"\n",
    "    EMBEDDING_DIM = int(os.getenv(\"EMBEDDING_DIM\", \"768\"))\n",
    "    \n",
    "    # Analysis Configuration\n",
    "    MIN_ALIGNMENT_SCORE = float(os.getenv(\"MIN_ALIGNMENT_SCORE\", \"0.5\"))\n",
    "    MAX_UNIT_COMBINATION = int(os.getenv(\"MAX_UNIT_COMBINATION\", \"3\"))\n",
    "    SIMILARITY_THRESHOLD = float(os.getenv(\"SIMILARITY_THRESHOLD\", \"0.8\"))\n",
    "    PARTIAL_THRESHOLD = float(os.getenv(\"PARTIAL_THRESHOLD\", \"0.6\"))\n",
    "    \n",
    "    # Scoring Weights\n",
    "    COVERAGE_WEIGHT = float(os.getenv(\"COVERAGE_WEIGHT\", \"0.5\"))\n",
    "    CONTEXT_WEIGHT = float(os.getenv(\"CONTEXT_WEIGHT\", \"0.25\"))\n",
    "    QUALITY_WEIGHT = float(os.getenv(\"QUALITY_WEIGHT\", \"0.15\"))\n",
    "    EDGE_PENALTY_WEIGHT = float(os.getenv(\"EDGE_PENALTY_WEIGHT\", \"0.1\"))\n",
    "    \n",
    "    # Edge Case Thresholds\n",
    "    CONTEXT_IMBALANCE_THRESHOLD = float(os.getenv(\"CONTEXT_IMBALANCE_THRESHOLD\", \"0.3\"))\n",
    "    BREADTH_RATIO_MIN = float(os.getenv(\"BREADTH_RATIO_MIN\", \"0.7\"))\n",
    "    BREADTH_RATIO_MAX = float(os.getenv(\"BREADTH_RATIO_MAX\", \"1.5\"))\n",
    "    \n",
    "    # Study Level Configuration\n",
    "    STUDY_LEVEL_WEIGHTS = {\n",
    "        \"introductory\": 0.2,\n",
    "        \"intermediate\": 0.4,\n",
    "        \"advanced\": 0.6,\n",
    "        \"specialized\": 0.8,\n",
    "        \"postgraduate\": 1.0\n",
    "    }\n",
    "    \n",
    "    STUDY_LEVEL_SKILL_MAPPING = {\n",
    "        \"introductory\": \"NOVICE\",\n",
    "        \"intermediate\": \"COMPETENT\",\n",
    "        \"advanced\": \"PROFICIENT\",\n",
    "        \"specialized\": \"EXPERT\",\n",
    "        \"postgraduate\": \"EXPERT\"\n",
    "    }\n",
    "    \n",
    "    # Credit Hour Configuration\n",
    "    CREDIT_POINT_TO_HOURS = float(os.getenv(\"CREDIT_POINT_TO_HOURS\", \"12.5\"))\n",
    "    HOUR_RATIO_MIN = float(os.getenv(\"HOUR_RATIO_MIN\", \"0.7\"))\n",
    "    HOUR_RATIO_MAX = float(os.getenv(\"HOUR_RATIO_MAX\", \"1.5\"))\n",
    "    \n",
    "    # Logging Configuration\n",
    "    LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
    "    LOG_FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    LOG_FILE = OUTPUT_DIR / \"analysis.log\"\n",
    "    \n",
    "    # Cache Configuration\n",
    "    ENABLE_CACHE = os.getenv(\"ENABLE_CACHE\", \"true\").lower() == \"true\"\n",
    "    CACHE_EXPIRY_DAYS = int(os.getenv(\"CACHE_EXPIRY_DAYS\", \"30\"))\n",
    "    \n",
    "    # Report Configuration\n",
    "    REPORT_FORMAT = os.getenv(\"REPORT_FORMAT\", \"json\")\n",
    "    INCLUDE_EDGE_CASES = os.getenv(\"INCLUDE_EDGE_CASES\", \"true\").lower() == \"true\"\n",
    "    MAX_RECOMMENDATIONS_PER_COURSE = int(os.getenv(\"MAX_RECOMMENDATIONS_PER_COURSE\", \"5\"))\n",
    "    \n",
    "    @classmethod\n",
    "    def get_config_dict(cls) -> dict:\n",
    "        \"\"\"Get configuration as dictionary\"\"\"\n",
    "        return {\n",
    "            \"min_alignment_score\": cls.MIN_ALIGNMENT_SCORE,\n",
    "            \"max_unit_combination\": cls.MAX_UNIT_COMBINATION,\n",
    "            \"similarity_threshold\": cls.SIMILARITY_THRESHOLD,\n",
    "            \"partial_threshold\": cls.PARTIAL_THRESHOLD,\n",
    "            \"use_vllm_batch\": cls.USE_VLLM_BATCH,\n",
    "            \"vllm_batch_size\": cls.VLLM_BATCH_SIZE,\n",
    "            \"weights\": {\n",
    "                \"coverage\": cls.COVERAGE_WEIGHT,\n",
    "                \"context\": cls.CONTEXT_WEIGHT,\n",
    "                \"quality\": cls.QUALITY_WEIGHT,\n",
    "                \"edge_penalty\": cls.EDGE_PENALTY_WEIGHT\n",
    "            },\n",
    "            \"thresholds\": {\n",
    "                \"context_imbalance\": cls.CONTEXT_IMBALANCE_THRESHOLD,\n",
    "                \"breadth_ratio_min\": cls.BREADTH_RATIO_MIN,\n",
    "                \"breadth_ratio_max\": cls.BREADTH_RATIO_MAX\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_azure_openai_config(cls) -> dict:\n",
    "        \"\"\"Get Azure OpenAI configuration as dictionary\"\"\"\n",
    "        return {\n",
    "            \"endpoint\": cls.AZURE_OPENAI_ENDPOINT,\n",
    "            \"deployment\": cls.AZURE_OPENAI_DEPLOYMENT,\n",
    "            \"api_key\": cls.AZURE_OPENAI_API_KEY,\n",
    "            \"api_version\": cls.AZURE_OPENAI_API_VERSION,\n",
    "            \"timeout\": cls.AZURE_OPENAI_TIMEOUT,\n",
    "            \"max_tokens\": cls.AZURE_OPENAI_MAX_TOKENS,\n",
    "            \"temperature\": cls.AZURE_OPENAI_TEMPERATURE\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vllm_config(cls) -> dict:\n",
    "        \"\"\"Get vLLM configuration as dictionary\"\"\"\n",
    "        return {\n",
    "            \"model_name\": cls.VLLM_MODEL_NAME,\n",
    "            \"number_gpus\": cls.VLLM_NUM_GPUS,\n",
    "            \"max_model_len\": cls.VLLM_MAX_MODEL_LEN,\n",
    "            \"use_batch\": cls.USE_VLLM_BATCH,\n",
    "            \"batch_size\": cls.VLLM_BATCH_SIZE,\n",
    "            \"max_batch_size\": cls.VLLM_MAX_BATCH_SIZE,\n",
    "            \"batch_timeout\": cls.VLLM_BATCH_TIMEOUT,\n",
    "            \"model_cache_dir\": cls.MODEL_CACHE_DIR,\n",
    "            \"external_model_dir\": cls.EXTERNAL_MODEL_DIR\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def save_config(cls, filepath: str = None):\n",
    "        \"\"\"Save configuration to file\"\"\"\n",
    "        import json\n",
    "        \n",
    "        if filepath is None:\n",
    "            filepath = cls.OUTPUT_DIR / \"config.json\"\n",
    "        \n",
    "        config_dict = cls.get_config_dict()\n",
    "        config_dict[\"azure_openai\"] = cls.get_azure_openai_config()\n",
    "        config_dict[\"vllm\"] = cls.get_vllm_config()\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_config(cls, filepath: str):\n",
    "        \"\"\"Load configuration from file\"\"\"\n",
    "        import json\n",
    "        \n",
    "        with open(filepath, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Update class attributes\n",
    "        for key, value in config.items():\n",
    "            if key == \"azure_openai\":\n",
    "                for azure_key, azure_value in value.items():\n",
    "                    attr_name = f\"AZURE_OPENAI_{azure_key.upper()}\"\n",
    "                    if hasattr(cls, attr_name):\n",
    "                        setattr(cls, attr_name, azure_value)\n",
    "            elif key == \"vllm\":\n",
    "                for vllm_key, vllm_value in value.items():\n",
    "                    if vllm_key == \"use_batch\":\n",
    "                        setattr(cls, \"USE_VLLM_BATCH\", vllm_value)\n",
    "                    elif vllm_key == \"batch_size\":\n",
    "                        setattr(cls, \"VLLM_BATCH_SIZE\", vllm_value)\n",
    "                    else:\n",
    "                        attr_name = f\"VLLM_{vllm_key.upper()}\"\n",
    "                        if hasattr(cls, attr_name):\n",
    "                            setattr(cls, attr_name, vllm_value)\n",
    "            elif hasattr(cls, key.upper()):\n",
    "                setattr(cls, key.upper(), value)\n",
    "\n",
    "    # Model configurations\n",
    "    EMBEDDING_MODELS = {\n",
    "        \"jinaai--jina-embeddings-v4\": {\n",
    "            \"model_id\": \"jinaai/jina-embeddings-v4\",\n",
    "            \"revision\": \"737fa5c46f0262ceba4a462ffa1c5bcf01da416f\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"embedding_dim\": 768\n",
    "        },\n",
    "        \"jinaai--jina-embeddings-v3\": {\n",
    "            \"model_id\": \"jinaai/jina-embeddings-v3\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"embedding_dim\": 1024\n",
    "        },\n",
    "        \"BAAI--bge-large-en-v1.5\": {\n",
    "            \"model_id\": \"BAAI/bge-large-en-v1.5\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"embedding_dim\": 1024\n",
    "        },\n",
    "        \"BAAI--bge-base-en-v1.5\": {\n",
    "            \"model_id\": \"BAAI/bge-base-en-v1.5\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"embedding_dim\": 768\n",
    "        },\n",
    "        \"sentence-transformers--all-MiniLM-L6-v2\": {\n",
    "            \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"embedding_dim\": 384\n",
    "        },\n",
    "        \"sentence-transformers--all-mpnet-base-v2\": {\n",
    "            \"model_id\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"embedding_dim\": 768\n",
    "        },\n",
    "        \"intfloat--e5-large-v2\": {\n",
    "            \"model_id\": \"intfloat/e5-large-v2\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"embedding_dim\": 1024\n",
    "        },\n",
    "        \"WhereIsAI--UAE-Large-V1\": {\n",
    "            \"model_id\": \"WhereIsAI/UAE-Large-V1\",\n",
    "            \"revision\": None,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"embedding_dim\": 1024\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Model configurations\n",
    "    MODELS = {\n",
    "        \"mistralai--Mistral-7B-Instruct-v0.2\": {\n",
    "            \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            \"revision\": \"41b61a33a2483885c981aa79e0df6b32407ed873\",\n",
    "            \"template\": \"Mistral\"\n",
    "        },\n",
    "        \"mistralai--Mistral-7B-Instruct-v0.3\": {\n",
    "            \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "            \"revision\": \"e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\",\n",
    "            \"template\": \"Mistral\"\n",
    "        },\n",
    "        \"neuralmagic--Meta-Llama-3.1-70B-Instruct-quantized.w4a16\": {\n",
    "            \"model_id\": \"neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16\",\n",
    "            \"revision\": \"8c670bcdb23f58a977e1440354beb7c3e455961d\",\n",
    "            \"template\": \"Llama\"\n",
    "        },\n",
    "        \"meta-llama--Llama-3.1-8B-Instruct\": {\n",
    "            \"model_id\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            \"revision\": \"0e9e39f249a16976918f6564b8830bc894c89659\",\n",
    "            \"template\": \"Llama\"\n",
    "        },\n",
    "        \"neuralmagic--Meta-Llama-3.1-70B-Instruct-FP8\": {\n",
    "            \"model_id\": \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\",\n",
    "            \"revision\": \"08b31c0f951f2227f6cdbc088cdb6fd139aecf0f\",\n",
    "            \"template\": \"Llama\"\n",
    "        },\n",
    "        \"microsoft--Phi-4-mini-instruct\": {\n",
    "            \"model_id\": \"microsoft/Phi-4-mini-instruct\",\n",
    "            \"revision\": \"c0fb9e74abda11b496b7907a9c6c9009a7a0488f\",\n",
    "            \"template\": \"Phi\"\n",
    "        },\n",
    "        \"cortecs--Llama-3.3-70B-Instruct-FP8-Dynamic\": {\n",
    "            \"model_id\": \"cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic\",\n",
    "            \"revision\": \"3722358cc2b990b22304489b2f87ef3bb876d6f6\",\n",
    "            \"template\": \"Llama\"\n",
    "        },\n",
    "        \"gpt-oss-120b\": {\n",
    "            \"model_id\": \"/Volumes/jsa_external_prod/external_vols/scratch/Scratch/Ehsan/Models/gpt-oss-120b\",\n",
    "            \"revision\": None,\n",
    "            \"template\": \"GPT\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, Config.LOG_LEVEL),\n",
    "    format=Config.LOG_FORMAT,\n",
    "    handlers=[\n",
    "        logging.FileHandler(Config.LOG_FILE),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "# Silence Py4J logs while keeping your app logs at INFO\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "834d4fd3-6798-4503-91f8-167b8c39719c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# EmbeddingInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f204ea93-8cf7-44f8-9718-faa40ae63660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interface for local embedding model integration with multi-GPU support\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Optional, Union, Dict\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "# from config import Config\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import sentence-transformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    logger.error(\"sentence-transformers not installed. Please install it for embedding support.\")\n",
    "\n",
    "\n",
    "class EmbeddingInterface:\n",
    "    \"\"\"Interface for local embedding model with multi-GPU support\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"jinaai--jina-embeddings-v4\",\n",
    "                 model_cache_dir: str = \"/home/ehsan/.cache/huggingface/hub\",\n",
    "                 external_model_dir: str = \"/Volumes/jsa_external_prod/external_vols/scratch/Scratch/Ehsan/Models\",\n",
    "                 device: str = \"cuda\",\n",
    "                 batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Initialize embedding interface with local model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model to use from MODELS dict\n",
    "            model_cache_dir: Directory for HuggingFace cache\n",
    "            external_model_dir: Directory containing pre-downloaded models\n",
    "            device: Device to run model on (cuda, cuda:0, cuda:1, cpu)\n",
    "            batch_size: Default batch size for encoding\n",
    "        \"\"\"\n",
    "        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            raise ImportError(\"sentence-transformers is required for embeddings. Install with: pip install sentence-transformers\")\n",
    "        \n",
    "        self.MODELS = Config.EMBEDDING_MODELS\n",
    "        self.model_name = model_name\n",
    "        self.model_cache_dir = Path(model_cache_dir)\n",
    "        self.external_model_dir = Path(external_model_dir)\n",
    "        self.device_str = device\n",
    "        self.default_batch_size = batch_size\n",
    "        \n",
    "        # Parse device string to handle specific GPU selection\n",
    "        if device.startswith(\"cuda\"):\n",
    "            if \":\" in device:\n",
    "                # Specific GPU like cuda:1\n",
    "                self.device_id = int(device.split(\":\")[1])\n",
    "            else:\n",
    "                # Default to cuda:0\n",
    "                self.device_id = 0\n",
    "            # Set CUDA device for this model\n",
    "            torch.cuda.set_device(self.device_id)\n",
    "            self.device = torch.device(f\"cuda:{self.device_id}\")\n",
    "        else:\n",
    "            self.device_id = None\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        logger.info(f\"Embedding interface will use device: {self.device}\")\n",
    "        \n",
    "        # Get model configuration\n",
    "        if model_name not in self.MODELS:\n",
    "            logger.warning(f\"Unknown model: {model_name}. Using default configuration.\")\n",
    "            self.model_config = {\n",
    "                \"model_id\": model_name,\n",
    "                \"revision\": None,\n",
    "                \"trust_remote_code\": True,\n",
    "                \"embedding_dim\": 768\n",
    "            }\n",
    "        else:\n",
    "            self.model_config = self.MODELS[model_name]\n",
    "        \n",
    "        self.embedding_dim = self.model_config.get(\"embedding_dim\", 768)\n",
    "        self.trust_remote_code = self.model_config.get(\"trust_remote_code\", False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = None\n",
    "        self._initialize_model()\n",
    "        \n",
    "        # Initialize cache\n",
    "        self.cache = {}\n",
    "        \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the SentenceTransformer model with proper device handling\"\"\"\n",
    "        try:\n",
    "            snapshot_location = self._get_snapshot_location()\n",
    "            logger.info(f\"Loading embedding model from: {snapshot_location}\")\n",
    "            \n",
    "            # Create a context manager to ensure model loads on correct device\n",
    "            with torch.cuda.device(self.device_id) if self.device_id is not None else torch.cuda.device(0):\n",
    "                # Initialize model - do NOT pass device parameter to avoid conflicts\n",
    "                self.model = SentenceTransformer(\n",
    "                    snapshot_location,\n",
    "                    trust_remote_code=self.trust_remote_code,\n",
    "                    device=self.device\n",
    "                )\n",
    "                \n",
    "                # Manually move model to the correct device\n",
    "                self.model = self.model.to(self.device)\n",
    "                \n",
    "                # Set the device in the model's internal state\n",
    "                self.model._target_device = self.device\n",
    "                \n",
    "                # Ensure all sub-modules are on the correct device\n",
    "                for module in self.model.modules():\n",
    "                    module.to(self.device)\n",
    "                \n",
    "                # If model has tokenizer, ensure it's configured correctly\n",
    "                if hasattr(self.model, 'tokenizer'):\n",
    "                    # Some tokenizers have device-specific settings\n",
    "                    if hasattr(self.model.tokenizer, 'padding_side'):\n",
    "                        self.model.tokenizer.padding_side = 'right'\n",
    "            \n",
    "            # Update embedding dimension from model\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            logger.info(f\"Successfully loaded embedding model: {self.model_name} on {self.device} (dim={self.embedding_dim})\")\n",
    "            \n",
    "            # Verify device placement\n",
    "            self._verify_device_placement()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize embedding model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _verify_device_placement(self):\n",
    "        \"\"\"Verify all model parameters are on the correct device\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.device != self.device:\n",
    "                logger.warning(f\"Parameter {name} is on {param.device}, moving to {self.device}\")\n",
    "                param.data = param.data.to(self.device)\n",
    "        \n",
    "        # Check buffers as well\n",
    "        for name, buffer in self.model.named_buffers():\n",
    "            if buffer.device != self.device:\n",
    "                logger.warning(f\"Buffer {name} is on {buffer.device}, moving to {self.device}\")\n",
    "                buffer.data = buffer.data.to(self.device)\n",
    "    \n",
    "    def _get_snapshot_location(self, copy_model: bool = True) -> str:\n",
    "        \"\"\"Get or download model snapshot location\"\"\"\n",
    "        model_id = self.model_config['model_id']\n",
    "        revision = self.model_config.get('revision')\n",
    "        \n",
    "        if revision:\n",
    "            external_path = self.external_model_dir / f\"models--{self.model_name}\"\n",
    "            cache_path = self.model_cache_dir / f\"models--{self.model_name}\"\n",
    "            \n",
    "            if copy_model and external_path.exists():\n",
    "                try:\n",
    "                    shutil.copytree(str(external_path), str(cache_path))\n",
    "                    logger.info(f\"Copied model from {external_path} to {cache_path}\")\n",
    "                except FileExistsError:\n",
    "                    logger.info(f\"Model already exists in cache: {cache_path}\")\n",
    "            \n",
    "            snapshot_location = snapshot_download(\n",
    "                repo_id=model_id,\n",
    "                revision=revision,\n",
    "                cache_dir=str(self.model_cache_dir)\n",
    "            )\n",
    "        else:\n",
    "            if copy_model:\n",
    "                external_path = self.external_model_dir / self.model_name\n",
    "                cache_path = self.model_cache_dir / self.model_name\n",
    "                \n",
    "                if external_path.exists():\n",
    "                    try:\n",
    "                        shutil.copytree(str(external_path), str(cache_path))\n",
    "                        logger.info(f\"Copied model from {external_path} to {cache_path}\")\n",
    "                    except FileExistsError:\n",
    "                        logger.info(f\"Model already exists in cache: {cache_path}\")\n",
    "                    \n",
    "                    snapshot_location = str(cache_path)\n",
    "                else:\n",
    "                    snapshot_location = snapshot_download(\n",
    "                        repo_id=model_id,\n",
    "                        cache_dir=str(self.model_cache_dir)\n",
    "                    )\n",
    "            else:\n",
    "                cache_path = self.model_cache_dir / self.model_name\n",
    "                if cache_path.exists():\n",
    "                    snapshot_location = str(cache_path)\n",
    "                else:\n",
    "                    snapshot_location = snapshot_download(\n",
    "                        repo_id=model_id,\n",
    "                        cache_dir=str(self.model_cache_dir)\n",
    "                    )\n",
    "        \n",
    "        return snapshot_location\n",
    "    \n",
    "    def encode(self, texts: Union[str, List[str]], \n",
    "               batch_size: Optional[int] = None,\n",
    "               show_progress: bool = False,\n",
    "               convert_to_tensor: bool = False,\n",
    "               normalize_embeddings: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for texts with proper device handling\n",
    "        \n",
    "        Args:\n",
    "            texts: Single text or list of texts to encode\n",
    "            batch_size: Batch size for encoding\n",
    "            show_progress: Whether to show progress bar\n",
    "            convert_to_tensor: Return PyTorch tensor instead of numpy\n",
    "            normalize_embeddings: Whether to normalize embeddings\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of embeddings (or tensor if convert_to_tensor=True)\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        if not texts:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Use cache for single texts\n",
    "        if len(texts) == 1 and texts[0] in self.cache and not convert_to_tensor:\n",
    "            return self.cache[texts[0]]\n",
    "        \n",
    "        # Set batch size\n",
    "        if batch_size is None:\n",
    "            batch_size = self.default_batch_size\n",
    "        \n",
    "        # Ensure we're in the right CUDA context\n",
    "        with torch.cuda.device(self.device_id) if self.device_id is not None else torch.cuda.device(0):\n",
    "            # Double-check model is on correct device\n",
    "            self.model._target_device = self.device\n",
    "            \n",
    "            # Use a custom encoding approach to ensure device consistency\n",
    "            try:\n",
    "                # For Jina models with task parameter\n",
    "                embeddings = self.model.encode(\n",
    "                    texts,\n",
    "                    task='text-matching',\n",
    "                    prompt_name='passage',\n",
    "                    batch_size=batch_size,\n",
    "                    show_progress_bar=show_progress,\n",
    "                    convert_to_tensor=convert_to_tensor,\n",
    "                    normalize_embeddings=normalize_embeddings\n",
    "                )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Custom encoding failed: {e}. Falling back to standard encode with device.\")\n",
    "                # Fallback to standard encode\n",
    "                embeddings = self.model.encode(\n",
    "                    texts,\n",
    "                    task='text-matching',\n",
    "                    prompt_name='passage',\n",
    "                    batch_size=batch_size,\n",
    "                    show_progress_bar=show_progress,\n",
    "                    convert_to_tensor=convert_to_tensor,\n",
    "                    normalize_embeddings=normalize_embeddings,\n",
    "                    device=self.device\n",
    "                )\n",
    "        \n",
    "        # Cache single text results\n",
    "        if len(texts) == 1 and not convert_to_tensor:\n",
    "            if isinstance(embeddings, np.ndarray):\n",
    "                self.cache[texts[0]] = embeddings[0] if embeddings.ndim > 1 else embeddings\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def encode_batch(self, texts: List[str], batch_size: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode multiple texts in batches (convenience method)\n",
    "        \"\"\"\n",
    "        return self.encode(texts, batch_size=batch_size)\n",
    "    \n",
    "    def similarity(self, embeddings1: np.ndarray, \n",
    "                  embeddings2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between embeddings\n",
    "        \"\"\"\n",
    "        # Handle single embedding\n",
    "        if embeddings1.ndim == 1:\n",
    "            embeddings1 = embeddings1.reshape(1, -1)\n",
    "        if embeddings2.ndim == 1:\n",
    "            embeddings2 = embeddings2.reshape(1, -1)\n",
    "        \n",
    "        # Use numpy operations to avoid device issues\n",
    "        # Normalize embeddings\n",
    "        norm1 = embeddings1 / (np.linalg.norm(embeddings1, axis=1, keepdims=True) + 1e-10)\n",
    "        norm2 = embeddings2 / (np.linalg.norm(embeddings2, axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity_matrix = np.dot(norm1, norm2.T)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def similarity_score(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate similarity score between two texts\"\"\"\n",
    "        embeddings = self.encode([text1, text2])\n",
    "        sim_matrix = self.similarity(embeddings[0:1], embeddings[1:2])\n",
    "        return float(sim_matrix[0, 0])\n",
    "    \n",
    "    def find_most_similar(self, query: str, \n",
    "                         candidates: List[str], \n",
    "                         top_k: int = 5) -> List[tuple]:\n",
    "        \"\"\"Find most similar texts from candidates\"\"\"\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        all_texts = [query] + candidates\n",
    "        embeddings = self.encode(all_texts)\n",
    "        \n",
    "        query_embedding = embeddings[0:1]\n",
    "        candidate_embeddings = embeddings[1:]\n",
    "        \n",
    "        similarities = self.similarity(query_embedding, candidate_embeddings)\n",
    "        scores = similarities[0]\n",
    "        \n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        results = [(candidates[idx], float(scores[idx])) for idx in top_indices]\n",
    "        return results\n",
    "    \n",
    "    def save_embeddings(self, embeddings: np.ndarray, \n",
    "                       texts: List[str], \n",
    "                       filepath: str):\n",
    "        \"\"\"Save embeddings to file\"\"\"\n",
    "        import json\n",
    "        \n",
    "        if torch.is_tensor(embeddings):\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        \n",
    "        data = {\n",
    "            \"embeddings\": embeddings.tolist(),\n",
    "            \"texts\": texts,\n",
    "            \"model\": self.model_name,\n",
    "            \"dimension\": self.embedding_dim,\n",
    "            \"device\": self.device_str\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        logger.info(f\"Saved {len(texts)} embeddings to {filepath}\")\n",
    "    \n",
    "    def load_embeddings(self, filepath: str) -> tuple:\n",
    "        \"\"\"Load embeddings from file\"\"\"\n",
    "        import json\n",
    "        \n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        embeddings = np.array(data[\"embeddings\"])\n",
    "        texts = data[\"texts\"]\n",
    "        \n",
    "        logger.info(f\"Loaded {len(texts)} embeddings from {filepath}\")\n",
    "        return embeddings, texts\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the embedding cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        logger.info(\"Embedding cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "678e1a9b-6393-4dec-b8c4-fa69587a1ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SkillExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fc34cb-5499-48f7-9254-30197ecfff6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced skill extraction from course descriptions using pure Gen AI\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from models.base_models import Skill, UnitOfCompetency, UniCourse\n",
    "from models.enums import SkillLevel, SkillContext, SkillCategory, StudyLevel\n",
    "from interfaces.genai_interface import GenAIInterface\n",
    "# from interfaces.embedding_interface import EmbeddingInterface\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SkillExtractor:\n",
    "    \"\"\"Advanced skill extraction using pure Gen AI approach\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 genai: Optional[GenAIInterface] = None,\n",
    "                 embeddings: Optional[EmbeddingInterface] = None,\n",
    "                 use_genai: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize skill extractor with Gen AI only approach\n",
    "        \n",
    "        Args:\n",
    "            genai: GenAI interface for advanced extraction\n",
    "            embeddings: Embedding interface for similarity\n",
    "            use_genai: Whether to use GenAI for extraction\n",
    "        \"\"\"\n",
    "        self.genai = genai\n",
    "        self.embeddings = embeddings\n",
    "        self.use_genai = use_genai and genai is not None\n",
    "        \n",
    "        # Cache for processed texts\n",
    "        self.cache = {}\n",
    "    \n",
    "    def extract_from_vet_unit(self, unit: UnitOfCompetency) -> List[Skill]:\n",
    "        \"\"\"\n",
    "        Extract skills from VET unit using Gen AI\n",
    "        \n",
    "        Args:\n",
    "            unit: VET unit to extract skills from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted skills\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting skills from VET unit: {unit.code}\")\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"vet_{unit.code}\"\n",
    "        if cache_key in self.cache:\n",
    "            unit.extracted_skills = self.cache[cache_key]\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        skills = []\n",
    "        \n",
    "        # Combine all text sources\n",
    "        full_text = unit.get_full_text()\n",
    "        \n",
    "        if self.use_genai:\n",
    "            # Step 1: Extract skills using Gen AI\n",
    "            ai_skills = self.genai.extract_skills_prompt(full_text, \"VET unit\")\n",
    "            skills.extend(self._convert_ai_skills(ai_skills, f\"VET:{unit.code}\"))\n",
    "            logger.debug(f\"AI extracted {len(ai_skills)} skills from {unit.code}\")\n",
    "            \n",
    "            # Step 2: Identify implicit skills\n",
    "            explicit_skill_names = [s[\"name\"] for s in ai_skills]\n",
    "            implicit_skills = self.genai.identify_implicit_skills(full_text, explicit_skill_names)\n",
    "            \n",
    "            for impl_skill in implicit_skills:\n",
    "                # Get category from AI and validate it\n",
    "                category_str = self.genai.categorize_skill(impl_skill[\"name\"], full_text)\n",
    "                category = self._validate_category(category_str)\n",
    "                \n",
    "                skill = Skill(\n",
    "                    name=impl_skill[\"name\"],\n",
    "                    category=category,\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.PRACTICAL,\n",
    "                    confidence=impl_skill.get(\"confidence\", 0.6),\n",
    "                    source=f\"VET:{unit.code}_implicit\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            \n",
    "            # Step 3: Handle composite skills\n",
    "            skill_names = [s.name for s in skills]\n",
    "            composite_result = self.genai.decompose_composite_skills(skill_names)\n",
    "            \n",
    "            for comp_skill in composite_result.get(\"composite_skills\", []):\n",
    "                if comp_skill.get(\"is_composite\"):\n",
    "                    for component in comp_skill.get(\"components\", []):\n",
    "                        # Validate category\n",
    "                        category = self._validate_category(component.get(\"category\", \"technical\"))\n",
    "                        \n",
    "                        new_skill = Skill(\n",
    "                            name=component[\"name\"],\n",
    "                            category=category,\n",
    "                            level=SkillLevel.COMPETENT,\n",
    "                            context=SkillContext.PRACTICAL,\n",
    "                            confidence=0.7,\n",
    "                            source=f\"VET:{unit.code}_decomposed\"\n",
    "                        )\n",
    "                        skills.append(new_skill)\n",
    "            \n",
    "            # Step 4: Determine context\n",
    "            context_result = self.genai.determine_context(full_text)\n",
    "            primary_context = context_result.get(\"context_analysis\", {}).get(\"primary_context\", \"practical\")\n",
    "            \n",
    "            for skill in skills:\n",
    "                if primary_context == \"practical\":\n",
    "                    skill.context = SkillContext.PRACTICAL\n",
    "                elif primary_context == \"theoretical\":\n",
    "                    skill.context = SkillContext.THEORETICAL\n",
    "                else:\n",
    "                    skill.context = SkillContext.HYBRID\n",
    "            \n",
    "            # Step 5: Deduplicate skills\n",
    "            skills = self._deduplicate_skills_with_ai(skills)\n",
    "        \n",
    "        else:\n",
    "            # Fallback: Create minimal skills from learning outcomes\n",
    "            logger.warning(f\"No GenAI available, using minimal extraction for {unit.code}\")\n",
    "            for outcome in unit.learning_outcomes[:5]:  # Limit to avoid too many\n",
    "                skill = Skill(\n",
    "                    name=self._clean_text(outcome)[:50],\n",
    "                    category=SkillCategory.TECHNICAL,\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.PRACTICAL,\n",
    "                    confidence=0.5,\n",
    "                    source=f\"VET:{unit.code}_fallback\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "        \n",
    "        # Cache and assign\n",
    "        self.cache[cache_key] = skills\n",
    "        unit.extracted_skills = skills\n",
    "        \n",
    "        logger.info(f\"Extracted {len(skills)} skills from {unit.code}\")\n",
    "        return skills\n",
    "    \n",
    "    def extract_from_uni_course(self, course: UniCourse) -> List[Skill]:\n",
    "        \"\"\"\n",
    "        Extract skills from university course using Gen AI\n",
    "        \n",
    "        Args:\n",
    "            course: University course to extract skills from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted skills\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting skills from Uni course: {course.code}\")\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"uni_{course.code}\"\n",
    "        if cache_key in self.cache:\n",
    "            course.extracted_skills = self.cache[cache_key]\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        skills = []\n",
    "        \n",
    "        # Combine all text sources\n",
    "        full_text = course.get_full_text()\n",
    "        \n",
    "        if self.use_genai:\n",
    "            # Step 1: Identify study level\n",
    "            if not course.study_level or course.study_level == \"intermediate\":\n",
    "                study_level_result = self.genai.identify_study_level(full_text)\n",
    "                course.study_level = study_level_result.get(\"study_level\", \"intermediate\")\n",
    "            \n",
    "            # Step 2: Extract skills using Gen AI\n",
    "            ai_skills = self.genai.extract_skills_prompt(full_text, \"university course\")\n",
    "            skills.extend(self._convert_ai_skills(ai_skills, f\"UNI:{course.code}\"))\n",
    "            logger.debug(f\"AI extracted {len(ai_skills)} skills from {course.code}\")\n",
    "            \n",
    "            # Step 3: Analyze prerequisites\n",
    "            if course.prerequisites:\n",
    "                prereq_result = self.genai.analyze_prerequisites(course.prerequisites, full_text)\n",
    "                for prereq in prereq_result.get(\"prerequisites\", []):\n",
    "                    for impl_skill in prereq.get(\"implied_skills\", []):\n",
    "                        skill = Skill(\n",
    "                            name=impl_skill[\"name\"],\n",
    "                            category=SkillCategory.FOUNDATIONAL,\n",
    "                            level=SkillLevel.from_string(impl_skill.get(\"level\", \"competent\")),\n",
    "                            context=SkillContext.THEORETICAL,\n",
    "                            confidence=0.8,\n",
    "                            source=f\"UNI:{course.code}_prerequisite\"\n",
    "                        )\n",
    "                        skills.append(skill)\n",
    "            \n",
    "            # Step 4: Analyze assessment context\n",
    "            if course.assessment:\n",
    "                assessment_result = self.genai.analyze_assessment(course.assessment)\n",
    "                assessment_context = assessment_result.get(\"primary_assessment_context\", \"hybrid\")\n",
    "                \n",
    "                for skill in skills:\n",
    "                    if assessment_context == \"practical\" and skill.context == SkillContext.THEORETICAL:\n",
    "                        skill.context = SkillContext.HYBRID\n",
    "            \n",
    "            # Step 5: Identify implicit skills\n",
    "            explicit_skill_names = [s.name for s in skills]\n",
    "            implicit_skills = self.genai.identify_implicit_skills(full_text, explicit_skill_names)\n",
    "            \n",
    "            for impl_skill in implicit_skills:\n",
    "                # Get category from AI and validate it\n",
    "                category_str = self.genai.categorize_skill(impl_skill[\"name\"], full_text)\n",
    "                category = self._validate_category(category_str)\n",
    "                \n",
    "                skill = Skill(\n",
    "                    name=impl_skill[\"name\"],\n",
    "                    category=category,\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.HYBRID,\n",
    "                    confidence=impl_skill.get(\"confidence\", 0.6),\n",
    "                    source=f\"UNI:{course.code}_implicit\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            \n",
    "            # Step 6: Adjust skill levels\n",
    "            skills_dict = [{\"name\": s.name, \"level\": s.level.name} for s in skills]\n",
    "            level_adjustment = self.genai.adjust_skill_levels(skills_dict, course.study_level, full_text)\n",
    "            \n",
    "            for adj_skill in level_adjustment.get(\"adjusted_skills\", []):\n",
    "                for skill in skills:\n",
    "                    if skill.name == adj_skill[\"skill_name\"]:\n",
    "                        skill.level = SkillLevel.from_string(adj_skill[\"adjusted_level\"])\n",
    "                        break\n",
    "            \n",
    "            # Step 7: Deduplicate skills\n",
    "            skills = self._deduplicate_skills_with_ai(skills)\n",
    "        \n",
    "        else:\n",
    "            # Fallback: Create minimal skills from learning outcomes\n",
    "            logger.warning(f\"No GenAI available, using minimal extraction for {course.code}\")\n",
    "            for outcome in course.learning_outcomes[:5]:\n",
    "                skill = Skill(\n",
    "                    name=self._clean_text(outcome)[:50],\n",
    "                    category=SkillCategory.TECHNICAL,\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.THEORETICAL,\n",
    "                    confidence=0.5,\n",
    "                    source=f\"UNI:{course.code}_fallback\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            \n",
    "            # Add skills from topics\n",
    "            for topic in course.topics[:5]:\n",
    "                skill = Skill(\n",
    "                    name=self._clean_text(topic)[:50],\n",
    "                    category=SkillCategory.TECHNICAL,\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.THEORETICAL,\n",
    "                    confidence=0.5,\n",
    "                    source=f\"UNI:{course.code}_topic\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "        \n",
    "        # Cache and assign\n",
    "        self.cache[cache_key] = skills\n",
    "        course.extracted_skills = skills\n",
    "        \n",
    "        logger.info(f\"Extracted {len(skills)} skills from {course.code}\")\n",
    "        return skills\n",
    "    \n",
    "    def _validate_category(self, category_str: str) -> SkillCategory:\n",
    "        \"\"\"Validate and convert category string to SkillCategory enum\"\"\"\n",
    "        valid_categories = {\n",
    "            \"technical\": SkillCategory.TECHNICAL,\n",
    "            \"cognitive\": SkillCategory.COGNITIVE,\n",
    "            \"practical\": SkillCategory.PRACTICAL,\n",
    "            \"foundational\": SkillCategory.FOUNDATIONAL,\n",
    "            \"professional\": SkillCategory.PROFESSIONAL\n",
    "        }\n",
    "        \n",
    "        category_lower = category_str.lower().strip()\n",
    "        \n",
    "        if category_lower in valid_categories:\n",
    "            return valid_categories[category_lower]\n",
    "        \n",
    "        # Try to find closest match\n",
    "        for valid_cat in valid_categories:\n",
    "            if valid_cat in category_lower or category_lower in valid_cat:\n",
    "                return valid_categories[valid_cat]\n",
    "        \n",
    "        # Default to technical\n",
    "        logger.debug(f\"Invalid category '{category_str}' defaulting to 'technical'\")\n",
    "        return SkillCategory.TECHNICAL\n",
    "    \n",
    "    def _validate_context(self, context_str: str) -> SkillContext:\n",
    "        \"\"\"Validate and convert context string to SkillContext enum\"\"\"\n",
    "        valid_contexts = {\n",
    "            \"theoretical\": SkillContext.THEORETICAL,\n",
    "            \"practical\": SkillContext.PRACTICAL,\n",
    "            \"hybrid\": SkillContext.HYBRID\n",
    "        }\n",
    "        \n",
    "        context_lower = context_str.lower().strip()\n",
    "        \n",
    "        if context_lower in valid_contexts:\n",
    "            return valid_contexts[context_lower]\n",
    "        \n",
    "        # Default to hybrid\n",
    "        logger.debug(f\"Invalid context '{context_str}' defaulting to 'hybrid'\")\n",
    "        return SkillContext.HYBRID\n",
    "    \n",
    "    def _convert_ai_skills(self, ai_skills: List[Dict], source: str) -> List[Skill]:\n",
    "        \"\"\"Convert AI-extracted skills to Skill objects\"\"\"\n",
    "        skills = []\n",
    "        \n",
    "        for ai_skill in ai_skills:\n",
    "            try:\n",
    "                skill_name = ai_skill.get(\"name\", \"\").strip()\n",
    "                if len(skill_name) < 3 or len(skill_name) > 100:\n",
    "                    continue\n",
    "                \n",
    "                # Validate category and context\n",
    "                category = self._validate_category(ai_skill.get(\"category\", \"technical\"))\n",
    "                context = self._validate_context(ai_skill.get(\"context\", \"hybrid\"))\n",
    "                \n",
    "                skill = Skill(\n",
    "                    name=skill_name,\n",
    "                    category=category,\n",
    "                    level=SkillLevel.from_string(ai_skill.get(\"level\", \"competent\")),\n",
    "                    context=context,\n",
    "                    keywords=ai_skill.get(\"keywords\", [])[:10],\n",
    "                    confidence=min(1.0, max(0.0, ai_skill.get(\"confidence\", 0.8))),\n",
    "                    source=source\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to convert AI skill: {e}\")\n",
    "        \n",
    "        return skills\n",
    "    \n",
    "    def _deduplicate_skills_with_ai(self, skills: List[Skill]) -> List[Skill]:\n",
    "        \"\"\"Deduplicate skills using Gen AI\"\"\"\n",
    "        if len(skills) <= 1:\n",
    "            return skills\n",
    "        \n",
    "        if not self.genai:\n",
    "            # Simple deduplication without AI\n",
    "            return self._simple_deduplicate(skills)\n",
    "        \n",
    "        # Convert skills for AI\n",
    "        skills_dict = [\n",
    "            {\n",
    "                \"name\": s.name,\n",
    "                \"category\": s.category.value,\n",
    "                \"level\": s.level.name,\n",
    "                \"confidence\": s.confidence\n",
    "            }\n",
    "            for s in skills\n",
    "        ]\n",
    "        \n",
    "        # Use AI to identify duplicates\n",
    "        dedup_result = self.genai.deduplicate_skills(skills_dict)\n",
    "        \n",
    "        # Build merged skills\n",
    "        final_skills = []\n",
    "        processed_names = set()\n",
    "        \n",
    "        # Process skill groups\n",
    "        for group in dedup_result.get(\"skill_groups\", []):\n",
    "            similar_names = [s.lower() for s in group.get(\"similar_skills\", [])]\n",
    "            merged_name = group.get(\"merged_name\", \"\")\n",
    "            \n",
    "            if merged_name:\n",
    "                best_skill = None\n",
    "                best_confidence = 0\n",
    "                \n",
    "                for skill in skills:\n",
    "                    if skill.name.lower() in similar_names:\n",
    "                        if skill.confidence > best_confidence:\n",
    "                            best_skill = skill\n",
    "                            best_confidence = skill.confidence\n",
    "                        processed_names.add(skill.name.lower())\n",
    "                \n",
    "                if best_skill:\n",
    "                    best_skill.name = merged_name\n",
    "                    # Validate category before setting\n",
    "                    merged_category = self._validate_category(group.get(\"merged_category\", best_skill.category.value))\n",
    "                    best_skill.category = merged_category\n",
    "                    best_skill.level = SkillLevel.from_string(group.get(\"merged_level\", best_skill.level.name))\n",
    "                    final_skills.append(best_skill)\n",
    "        \n",
    "        # Add unique skills\n",
    "        for skill_name in dedup_result.get(\"unique_skills\", []):\n",
    "            for skill in skills:\n",
    "                if skill.name == skill_name and skill.name.lower() not in processed_names:\n",
    "                    final_skills.append(skill)\n",
    "                    processed_names.add(skill.name.lower())\n",
    "        \n",
    "        # Add any remaining skills\n",
    "        for skill in skills:\n",
    "            if skill.name.lower() not in processed_names:\n",
    "                final_skills.append(skill)\n",
    "        \n",
    "        return final_skills\n",
    "    \n",
    "    def _simple_deduplicate(self, skills: List[Skill]) -> List[Skill]:\n",
    "        \"\"\"Simple deduplication without AI\"\"\"\n",
    "        skill_dict = {}\n",
    "        \n",
    "        for skill in skills:\n",
    "            key = skill.name.lower().strip()\n",
    "            \n",
    "            if key not in skill_dict:\n",
    "                skill_dict[key] = skill\n",
    "            else:\n",
    "                # Keep the one with higher confidence\n",
    "                if skill.confidence > skill_dict[key].confidence:\n",
    "                    skill_dict[key] = skill\n",
    "        \n",
    "        return list(skill_dict.values())\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text for skill name\"\"\"\n",
    "        text = \" \".join(text.split())  # Remove extra whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove common prefixes\n",
    "        prefixes = [\"ability to\", \"knowledge of\", \"understanding of\", \"skills in\"]\n",
    "        text_lower = text.lower()\n",
    "        for prefix in prefixes:\n",
    "            if text_lower.startswith(prefix):\n",
    "                text = text[len(prefix):].strip()\n",
    "        \n",
    "        return text[:100]  # Limit length\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the extraction cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        logger.info(\"Extraction cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e12e4ab2-5f04-44ce-93d5-2bf678e6a4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# VLLMGenAIInterfaceBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518e2426-048a-4ca6-934c-205be13dad32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interface for local GenAI model integration using vLLM with true batch processing\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "import torch\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "# from config import Config\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VLLMGenAIInterfaceBatch:\n",
    "    \"\"\"Interface for local GenAI model integration using vLLM with batch processing\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"meta-llama--Llama-3.1-8B-Instruct\",\n",
    "                 number_gpus: int = 1,\n",
    "                 max_model_len: int = 8192,\n",
    "                 batch_size: int = 8,\n",
    "                 model_cache_dir: str = \"/root/.cache/huggingface/hub\",\n",
    "                 external_model_dir: str = \"/Volumes/jsa_external_prod/external_vols/scratch/Scratch/Ehsan/Models\",\n",
    "                 gpu_id: int = 0):  # Add explicit GPU ID parameter\n",
    "        \"\"\"\n",
    "        Initialize vLLM GenAI interface with batch processing\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model to use\n",
    "            number_gpus: Number of GPUs for tensor parallelism\n",
    "            max_model_len: Maximum model context length\n",
    "            batch_size: Default batch size for processing\n",
    "            model_cache_dir: Directory for HuggingFace cache\n",
    "            external_model_dir: Directory containing pre-downloaded models\n",
    "            gpu_id: GPU ID to use (default 0)\n",
    "        \"\"\"\n",
    "        self.MODELS = Config.MODELS\n",
    "        self.model_name = model_name\n",
    "        self.number_gpus = number_gpus\n",
    "        self.max_model_len = max_model_len\n",
    "        self.batch_size = batch_size\n",
    "        self.model_cache_dir = Path(model_cache_dir)\n",
    "        self.external_model_dir = Path(external_model_dir)\n",
    "        self.gpu_id = gpu_id\n",
    "        \n",
    "        # Set environment variable to control GPU visibility for vLLM\n",
    "        if self.number_gpus == 1:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "            logger.info(f\"Set CUDA_VISIBLE_DEVICES={gpu_id} for vLLM batch interface\")\n",
    "        else:\n",
    "            # For multi-GPU, set the range starting from gpu_id\n",
    "            gpu_list = \",\".join(str(gpu_id + i) for i in range(number_gpus))\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_list\n",
    "            logger.info(f\"Set CUDA_VISIBLE_DEVICES={gpu_list} for vLLM batch interface\")\n",
    "        \n",
    "        # Get model configuration\n",
    "        if model_name not in self.MODELS:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        self.model_config = self.MODELS[model_name]\n",
    "        self.template = self.model_config.get(\"template\", \"Mistral\")\n",
    "        \n",
    "        # Import prompts\n",
    "        from extraction.genai_prompts import GenAIPrompts\n",
    "        self.prompts = GenAIPrompts()\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.llm = None\n",
    "        self._initialize_model()\n",
    "        \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the vLLM model\"\"\"\n",
    "        try:\n",
    "            snapshot_location = self._get_snapshot_location()\n",
    "            logger.info(f\"Loading model from: {snapshot_location}\")\n",
    "            \n",
    "            # Initialize vLLM (it will use the GPUs specified by CUDA_VISIBLE_DEVICES)\n",
    "            self.llm = LLM(\n",
    "                model=snapshot_location,\n",
    "                tensor_parallel_size=self.number_gpus,\n",
    "                max_model_len=self.max_model_len,\n",
    "                gpu_memory_utilization=0.9  # Allow vLLM to use 90% of GPU memory\n",
    "            )\n",
    "            logger.info(f\"Successfully loaded model: {self.model_name} on GPU(s) specified by CUDA_VISIBLE_DEVICES\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize model: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def _get_snapshot_location(self, copy_model: bool = False) -> str:\n",
    "        \"\"\"Get or download model snapshot location\"\"\"\n",
    "        model_id = self.model_config['model_id']\n",
    "        revision = self.model_config['revision']\n",
    "        \n",
    "        if revision:\n",
    "            external_path = self.external_model_dir / f\"models--{self.model_name}\"\n",
    "            cache_path = self.model_cache_dir / f\"models--{self.model_name}\"\n",
    "            \n",
    "            if copy_model and external_path.exists():\n",
    "                try:\n",
    "                    shutil.copytree(str(external_path), str(cache_path))\n",
    "                    logger.info(f\"Copied model from {external_path} to {cache_path}\")\n",
    "                except FileExistsError:\n",
    "                    logger.info(f\"Model already exists in cache: {cache_path}\")\n",
    "            \n",
    "            snapshot_location = snapshot_download(\n",
    "                repo_id=model_id,\n",
    "                revision=revision,\n",
    "                cache_dir=str(self.model_cache_dir)\n",
    "            )\n",
    "        else:\n",
    "            if copy_model:\n",
    "                external_path = self.external_model_dir / self.model_name\n",
    "                cache_path = self.model_cache_dir / self.model_name\n",
    "                \n",
    "                if external_path.exists():\n",
    "                    try:\n",
    "                        shutil.copytree(str(external_path), str(cache_path))\n",
    "                        logger.info(f\"Copied model from {external_path} to {cache_path}\")\n",
    "                    except FileExistsError:\n",
    "                        logger.info(f\"Model already exists in cache: {cache_path}\")\n",
    "                \n",
    "                snapshot_location = str(cache_path)\n",
    "            else:\n",
    "                snapshot_location = model_id\n",
    "        \n",
    "        return snapshot_location\n",
    "    \n",
    "    def _format_instruction(self, sys_message: str, query: str) -> str:\n",
    "        \"\"\"Format instruction based on model template\"\"\"\n",
    "        if self.template == 'Phi':\n",
    "            return f'<|system|> {sys_message} <|end|><|user|> {query} <|end|><|assistant|>'\n",
    "        elif self.template == 'Llama':\n",
    "            return f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>{sys_message}<|eot_id|><|start_header_id|>user<|end_header_id|>{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "        elif self.template == \"GPT\":\n",
    "            return f'''<|start|>system<|message|>{sys_message}\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>{query}<|end|><|start|>assistant'''\n",
    "        else:  # Default Mistral format\n",
    "            return f'<s> [INST] {sys_message} [/INST]\\nUser: {query}\\nAssistant: '\n",
    "    \n",
    "    def _generate_batch(self, system_prompt: str, user_prompts: List[str], max_tokens: int = 2048) -> List[str]:\n",
    "        \"\"\"Generate responses for a batch of prompts\"\"\"\n",
    "        full_prompts = [\n",
    "            self._format_instruction(system_prompt, user_prompt) \n",
    "            for user_prompt in user_prompts\n",
    "        ]\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.0,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            n=1,\n",
    "            best_of=1\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate(full_prompts, sampling_params=sampling_params)\n",
    "        return [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "    def _parse_json_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse JSON from model response\"\"\"\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.warning(f\"Failed to parse JSON response: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Batch extraction methods\n",
    "    \n",
    "    def extract_skills_batch(self, texts: List[str], contexts: List[str] = None) -> List[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Extract skills from multiple texts in batch\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to extract skills from\n",
    "            contexts: List of context types (optional)\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted skills for each text\n",
    "        \"\"\"\n",
    "        if contexts is None:\n",
    "            contexts = [\"course\"] * len(texts)\n",
    "        \n",
    "        system_prompt = self.prompts.skill_extraction_prompt()\n",
    "        user_prompts = [\n",
    "            f\"Context: {context}\\n\\nText to analyze:\\n{text[:3000]}\"\n",
    "            for text, context in zip(texts, contexts)\n",
    "        ]\n",
    "        \n",
    "        # Process in batches if needed\n",
    "        all_results = []\n",
    "        for i in range(0, len(user_prompts), self.batch_size):\n",
    "            batch = user_prompts[i:i + self.batch_size]\n",
    "            responses = self._generate_batch(system_prompt, batch)\n",
    "            \n",
    "            for response in responses:\n",
    "                result = self._parse_json_response(response)\n",
    "                all_results.append(result.get(\"skills\", []))\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def identify_study_levels_batch(self, course_texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Identify study levels for multiple courses\"\"\"\n",
    "        system_prompt = self.prompts.study_level_identification_prompt()\n",
    "        user_prompts = [\n",
    "            f\"Course description:\\n{text[:2000]}\" \n",
    "            for text in course_texts\n",
    "        ]\n",
    "        \n",
    "        all_results = []\n",
    "        for i in range(0, len(user_prompts), self.batch_size):\n",
    "            batch = user_prompts[i:i + self.batch_size]\n",
    "            responses = self._generate_batch(system_prompt, batch, max_tokens=512)\n",
    "            \n",
    "            for response in responses:\n",
    "                all_results.append(self._parse_json_response(response))\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def identify_implicit_skills_batch(self, texts: List[str], explicit_skills_lists: List[List[str]]) -> List[List[Dict]]:\n",
    "        \"\"\"Identify implicit skills for multiple texts\"\"\"\n",
    "        system_prompt = self.prompts.implicit_skill_identification_prompt()\n",
    "        user_prompts = [\n",
    "            f\"Course content: {text[:1500]}\\n\\nExplicit skills already identified: {', '.join(skills[:20])}\"\n",
    "            for text, skills in zip(texts, explicit_skills_lists)\n",
    "        ]\n",
    "        \n",
    "        all_results = []\n",
    "        for i in range(0, len(user_prompts), self.batch_size):\n",
    "            batch = user_prompts[i:i + self.batch_size]\n",
    "            responses = self._generate_batch(system_prompt, batch)\n",
    "            \n",
    "            for response in responses:\n",
    "                result = self._parse_json_response(response)\n",
    "                all_results.append(result.get(\"implicit_skills\", []))\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def determine_contexts_batch(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Determine theoretical vs practical context for multiple texts\"\"\"\n",
    "        system_prompt = self.prompts.context_determination_prompt()\n",
    "        user_prompts = [f\"Text to analyze:\\n{text[:2000]}\" for text in texts]\n",
    "        \n",
    "        all_results = []\n",
    "        for i in range(0, len(user_prompts), self.batch_size):\n",
    "            batch = user_prompts[i:i + self.batch_size]\n",
    "            responses = self._generate_batch(system_prompt, batch, max_tokens=512)\n",
    "            \n",
    "            for response in responses:\n",
    "                all_results.append(self._parse_json_response(response))\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def analyze_skill_similarities_batch(self, skill_pairs: List[tuple]) -> List[float]:\n",
    "        \"\"\"Analyze similarity for multiple skill pairs\"\"\"\n",
    "        system_prompt = self.prompts.skill_similarity_prompt()\n",
    "        user_prompts = [\n",
    "            f\"Skill 1: {skill1}\\nSkill 2: {skill2}\"\n",
    "            for skill1, skill2 in skill_pairs\n",
    "        ]\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in range(0, len(user_prompts), self.batch_size):\n",
    "            batch = user_prompts[i:i + self.batch_size]\n",
    "            responses = self._generate_batch(system_prompt, batch, max_tokens=256)\n",
    "            \n",
    "            for response in responses:\n",
    "                result = self._parse_json_response(response)\n",
    "                all_scores.append(result.get(\"similarity_score\", 0.5))\n",
    "        \n",
    "        return all_scores\n",
    "    \n",
    "    # Single-item methods that delegate to batch methods\n",
    "    \n",
    "    def extract_skills_prompt(self, text: str, context: str = \"course\") -> List[Dict]:\n",
    "        \"\"\"Extract skills from a single text (delegates to batch)\"\"\"\n",
    "        results = self.extract_skills_batch([text], [context])\n",
    "        return results[0] if results else []\n",
    "    \n",
    "    def identify_study_level(self, course_text: str) -> Dict:\n",
    "        \"\"\"Identify study level from single course (delegates to batch)\"\"\"\n",
    "        results = self.identify_study_levels_batch([course_text])\n",
    "        return results[0] if results else {}\n",
    "    \n",
    "    def identify_implicit_skills(self, text: str, explicit_skills: List[str]) -> List[Dict]:\n",
    "        \"\"\"Identify implicit skills for single text (delegates to batch)\"\"\"\n",
    "        results = self.identify_implicit_skills_batch([text], [explicit_skills])\n",
    "        return results[0] if results else []\n",
    "    \n",
    "    def determine_context(self, text: str) -> Dict:\n",
    "        \"\"\"Determine context for single text (delegates to batch)\"\"\"\n",
    "        results = self.determine_contexts_batch([text])\n",
    "        return results[0] if results else {}\n",
    "    \n",
    "    def analyze_skill_similarity(self, skill1: str, skill2: str) -> float:\n",
    "        \"\"\"Analyze similarity between two skills (delegates to batch)\"\"\"\n",
    "        results = self.analyze_skill_similarities_batch([(skill1, skill2)])\n",
    "        return results[0] if results else 0.5\n",
    "    \n",
    "    # Methods that remain single-processing (less common operations)\n",
    "    \n",
    "    def deduplicate_skills(self, skills: List[Dict]) -> Dict:\n",
    "        \"\"\"Deduplicate and merge similar skills\"\"\"\n",
    "        system_prompt = self.prompts.skill_deduplication_prompt()\n",
    "        skills_json = json.dumps(skills[:50], indent=2)\n",
    "        user_prompt = f\"Skills to analyze:\\n{skills_json}\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def decompose_composite_skills(self, skills: List[str]) -> Dict:\n",
    "        \"\"\"Decompose composite skills into components\"\"\"\n",
    "        system_prompt = self.prompts.composite_skill_decomposition_prompt()\n",
    "        user_prompt = f\"Skills to analyze:\\n{json.dumps(skills[:30], indent=2)}\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def adjust_skill_levels(self, skills: List[Dict], study_level: str, course_text: str) -> Dict:\n",
    "        \"\"\"Adjust skill levels based on context\"\"\"\n",
    "        system_prompt = self.prompts.skill_level_adjustment_prompt()\n",
    "        user_prompt = f\"\"\"Study level: {study_level}\n",
    "Course context: {course_text[:1000]}\n",
    "Skills to adjust: {json.dumps(skills[:30], indent=2)}\"\"\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def extract_technology_versions(self, text: str) -> Dict:\n",
    "        \"\"\"Extract technology versions and assess currency\"\"\"\n",
    "        system_prompt = self.prompts.technology_version_extraction_prompt()\n",
    "        user_prompt = f\"Text to analyze:\\n{text[:2000]}\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def analyze_prerequisites(self, prerequisites: List[str], course_text: str) -> Dict:\n",
    "        \"\"\"Analyze prerequisites and dependencies\"\"\"\n",
    "        system_prompt = self.prompts.prerequisite_analysis_prompt()\n",
    "        user_prompt = f\"\"\"Prerequisites: {', '.join(prerequisites)}\n",
    "Course context: {course_text[:1000]}\"\"\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def detect_edge_cases(self, vet_text: str, uni_text: str, mapping_info: Dict) -> Dict:\n",
    "        \"\"\"Detect edge cases in credit mapping\"\"\"\n",
    "        system_prompt = self.prompts.edge_case_detection_prompt()\n",
    "        user_prompt = f\"\"\"VET content: {vet_text[:1000]}\n",
    "University content: {uni_text[:1000]}\n",
    "Mapping summary: {json.dumps(mapping_info, indent=2)}\"\"\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def extract_keywords(self, skill_name: str, context_text: str) -> List[str]:\n",
    "        \"\"\"Extract relevant keywords for a skill\"\"\"\n",
    "        system_prompt = self.prompts.keyword_extraction_prompt()\n",
    "        user_prompt = f\"Skill: {skill_name}\\nContext: {context_text[:500]}\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt], max_tokens=256)[0]\n",
    "        result = self._parse_json_response(response)\n",
    "        return result.get(\"keywords\", [])\n",
    "    \n",
    "    def analyze_assessment(self, assessment_text: str) -> Dict:\n",
    "        \"\"\"Analyze assessment methods\"\"\"\n",
    "        system_prompt = self.prompts.assessment_type_analysis_prompt()\n",
    "        user_prompt = f\"Assessment description:\\n{assessment_text[:1500]}\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt])[0]\n",
    "        return self._parse_json_response(response)\n",
    "    \n",
    "    def categorize_skill(self, skill_name: str, context: str = \"\") -> str:\n",
    "        \"\"\"Categorize a skill\"\"\"\n",
    "        system_prompt = self.prompts.skill_categorization_prompt()\n",
    "        user_prompt = f\"Skill to categorize: {skill_name}\\nContext: {context[:200]}\"\n",
    "        \n",
    "        response = self._generate_batch(system_prompt, [user_prompt], max_tokens=256)[0]\n",
    "        result = self._parse_json_response(response)\n",
    "        return result.get(\"category\", \"technical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "458c1be6-1330-4a21-96ac-8b40dbb3b43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# VLLMSkillExtractorBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5edd8a6c-ac76-49df-a171-151a0f9d60e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vLLM skill extraction using true batch processing for efficient multi-unit/course processing\n",
    "\"\"\"\n",
    "\n",
    "# import logging\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "from models.base_models import Skill, UnitOfCompetency, UniCourse, VETQualification, UniQualification\n",
    "from models.enums import SkillLevel, SkillContext, SkillCategory, StudyLevel\n",
    "# from interfaces.embedding_interface import EmbeddingInterface\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VLLMSkillExtractorBatch:\n",
    "    \"\"\"vLLM skill extraction using true batch processing\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 genai=None,\n",
    "                 embeddings: Optional[EmbeddingInterface] = None):\n",
    "        \"\"\"\n",
    "        Initialize vLLM skill extractor with batch processing\n",
    "        \n",
    "        Args:\n",
    "            genai: vLLM GenAI batch interface for extraction\n",
    "            embeddings: Embedding interface for similarity\n",
    "        \"\"\"\n",
    "        self.genai = genai\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        # Cache for processed texts\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Batch processing size\n",
    "        self.batch_size = getattr(genai, 'batch_size', 8) if genai else 8\n",
    "    \n",
    "    def extract_from_vet_qualification(self, vet_qual: VETQualification) -> Dict[str, List[Skill]]:\n",
    "        \"\"\"\n",
    "        Extract skills from entire VET qualification using batch processing\n",
    "        \n",
    "        Args:\n",
    "            vet_qual: VET qualification to process\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping unit codes to extracted skills\n",
    "        \"\"\"\n",
    "        logger.info(f\"Batch extracting skills from VET qualification: {vet_qual.code}\")\n",
    "        \n",
    "        # Check which units need processing\n",
    "        units_to_process = []\n",
    "        units_cached = []\n",
    "        \n",
    "        for unit in vet_qual.units:\n",
    "            cache_key = f\"vet_{unit.code}\"\n",
    "            if cache_key in self.cache:\n",
    "                unit.extracted_skills = self.cache[cache_key]\n",
    "                units_cached.append(unit)\n",
    "            else:\n",
    "                units_to_process.append(unit)\n",
    "        \n",
    "        if units_cached:\n",
    "            logger.info(f\"Using cached skills for {len(units_cached)} units\")\n",
    "        \n",
    "        if units_to_process:\n",
    "            logger.info(f\"Processing {len(units_to_process)} VET units in batches\")\n",
    "            \n",
    "            # Process all units in batches\n",
    "            for i in range(0, len(units_to_process), self.batch_size):\n",
    "                batch_units = units_to_process[i:i + self.batch_size]\n",
    "                self._process_vet_batch(batch_units)\n",
    "        \n",
    "        # Return all skills\n",
    "        all_skills = {}\n",
    "        for unit in vet_qual.units:\n",
    "            all_skills[unit.code] = unit.extracted_skills\n",
    "        \n",
    "        logger.info(f\"Extracted skills from {len(all_skills)} units\")\n",
    "        return all_skills\n",
    "    \n",
    "    def extract_from_uni_qualification(self, uni_qual: UniQualification) -> Dict[str, List[Skill]]:\n",
    "        \"\"\"\n",
    "        Extract skills from entire university qualification using batch processing\n",
    "        \n",
    "        Args:\n",
    "            uni_qual: University qualification to process\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping course codes to extracted skills\n",
    "        \"\"\"\n",
    "        logger.info(f\"Batch extracting skills from Uni qualification: {uni_qual.code}\")\n",
    "        \n",
    "        # Check which courses need processing\n",
    "        courses_to_process = []\n",
    "        courses_cached = []\n",
    "        \n",
    "        for course in uni_qual.courses:\n",
    "            cache_key = f\"uni_{course.code}\"\n",
    "            if cache_key in self.cache:\n",
    "                course.extracted_skills = self.cache[cache_key]\n",
    "                courses_cached.append(course)\n",
    "            else:\n",
    "                courses_to_process.append(course)\n",
    "        \n",
    "        if courses_cached:\n",
    "            logger.info(f\"Using cached skills for {len(courses_cached)} courses\")\n",
    "        \n",
    "        if courses_to_process:\n",
    "            logger.info(f\"Processing {len(courses_to_process)} Uni courses in batches\")\n",
    "            \n",
    "            # Process all courses in batches\n",
    "            for i in range(0, len(courses_to_process), self.batch_size):\n",
    "                batch_courses = courses_to_process[i:i + self.batch_size]\n",
    "                self._process_uni_batch(batch_courses)\n",
    "        \n",
    "        # Return all skills\n",
    "        all_skills = {}\n",
    "        for course in uni_qual.courses:\n",
    "            all_skills[course.code] = course.extracted_skills\n",
    "        \n",
    "        logger.info(f\"Extracted skills from {len(all_skills)} courses\")\n",
    "        return all_skills\n",
    "    \n",
    "    def _process_vet_batch(self, units: List[UnitOfCompetency]):\n",
    "        \"\"\"Process a batch of VET units\"\"\"\n",
    "        # Step 1: Extract skills using batch GenAI\n",
    "        texts = [unit.get_full_text() for unit in units]\n",
    "        contexts = [\"VET unit\"] * len(units)\n",
    "        \n",
    "        ai_skills_batch = self.genai.extract_skills_batch(texts, contexts)\n",
    "        \n",
    "        # Step 2: Identify implicit skills in batch\n",
    "        explicit_skills_lists = [[s[\"name\"] for s in skills] for skills in ai_skills_batch]\n",
    "        implicit_skills_batch = self.genai.identify_implicit_skills_batch(texts, explicit_skills_lists)\n",
    "        \n",
    "        # Step 3: Determine contexts in batch\n",
    "        context_results = self.genai.determine_contexts_batch(texts)\n",
    "        \n",
    "        # Step 4: Process each unit with batch results\n",
    "        for idx, unit in enumerate(units):\n",
    "            skills = []\n",
    "            \n",
    "            # Add explicit skills\n",
    "            ai_skills = ai_skills_batch[idx] if idx < len(ai_skills_batch) else []\n",
    "            skills.extend(self._convert_ai_skills(ai_skills, f\"VET:{unit.code}\"))\n",
    "            \n",
    "            # Add implicit skills\n",
    "            implicit_skills = implicit_skills_batch[idx] if idx < len(implicit_skills_batch) else []\n",
    "            for impl_skill in implicit_skills:\n",
    "                skill = Skill(\n",
    "                    name=impl_skill[\"name\"],\n",
    "                    category=self._validate_category(impl_skill.get(\"category\", \"technical\")),\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.PRACTICAL,\n",
    "                    confidence=impl_skill.get(\"confidence\", 0.6),\n",
    "                    source=f\"VET:{unit.code}_implicit\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            \n",
    "            # Apply context\n",
    "            context_result = context_results[idx] if idx < len(context_results) else {}\n",
    "            primary_context = context_result.get(\"context_analysis\", {}).get(\"primary_context\", \"practical\")\n",
    "            \n",
    "            for skill in skills:\n",
    "                if primary_context == \"practical\":\n",
    "                    skill.context = SkillContext.PRACTICAL\n",
    "                elif primary_context == \"theoretical\":\n",
    "                    skill.context = SkillContext.THEORETICAL\n",
    "                else:\n",
    "                    skill.context = SkillContext.HYBRID\n",
    "            \n",
    "            # Step 5: Decompose composite skills (done per unit for now)\n",
    "            skill_names = [s.name for s in skills]\n",
    "            if skill_names:\n",
    "                composite_result = self.genai.decompose_composite_skills(skill_names)\n",
    "                \n",
    "                for comp_skill in composite_result.get(\"composite_skills\", []):\n",
    "                    if comp_skill.get(\"is_composite\"):\n",
    "                        for component in comp_skill.get(\"components\", []):\n",
    "                            new_skill = Skill(\n",
    "                                name=component[\"name\"],\n",
    "                                category=self._validate_category(component.get(\"category\", \"technical\")),\n",
    "                                level=SkillLevel.COMPETENT,\n",
    "                                context=SkillContext.PRACTICAL,\n",
    "                                confidence=0.7,\n",
    "                                source=f\"VET:{unit.code}_decomposed\"\n",
    "                            )\n",
    "                            skills.append(new_skill)\n",
    "            \n",
    "            # Step 6: Deduplicate skills\n",
    "            skills = self._deduplicate_skills_with_ai(skills)\n",
    "            \n",
    "            # Cache and assign\n",
    "            cache_key = f\"vet_{unit.code}\"\n",
    "            self.cache[cache_key] = skills\n",
    "            unit.extracted_skills = skills\n",
    "            \n",
    "            logger.debug(f\"Extracted {len(skills)} skills from {unit.code}\")\n",
    "    \n",
    "    def _process_uni_batch(self, courses: List[UniCourse]):\n",
    "        \"\"\"Process a batch of University courses\"\"\"\n",
    "        # Step 1: Identify study levels in batch\n",
    "        texts = [course.get_full_text() for course in courses]\n",
    "        study_level_results = self.genai.identify_study_levels_batch(texts)\n",
    "        \n",
    "        for idx, course in enumerate(courses):\n",
    "            if not course.study_level or course.study_level == \"intermediate\":\n",
    "                if idx < len(study_level_results):\n",
    "                    course.study_level = study_level_results[idx].get(\"study_level\", \"intermediate\")\n",
    "        \n",
    "        # Step 2: Extract skills using batch GenAI\n",
    "        contexts = [\"university course\"] * len(courses)\n",
    "        ai_skills_batch = self.genai.extract_skills_batch(texts, contexts)\n",
    "        \n",
    "        # Step 3: Identify implicit skills in batch\n",
    "        explicit_skills_lists = [[s[\"name\"] for s in skills] for skills in ai_skills_batch]\n",
    "        implicit_skills_batch = self.genai.identify_implicit_skills_batch(texts, explicit_skills_lists)\n",
    "        \n",
    "        # Step 4: Determine contexts in batch\n",
    "        context_results = self.genai.determine_contexts_batch(texts)\n",
    "        \n",
    "        # Step 5: Process each course with batch results\n",
    "        for idx, course in enumerate(courses):\n",
    "            skills = []\n",
    "            \n",
    "            # Add explicit skills\n",
    "            ai_skills = ai_skills_batch[idx] if idx < len(ai_skills_batch) else []\n",
    "            skills.extend(self._convert_ai_skills(ai_skills, f\"UNI:{course.code}\"))\n",
    "            \n",
    "            # Add implicit skills\n",
    "            implicit_skills = implicit_skills_batch[idx] if idx < len(implicit_skills_batch) else []\n",
    "            for impl_skill in implicit_skills:\n",
    "                skill = Skill(\n",
    "                    name=impl_skill[\"name\"],\n",
    "                    category=self._validate_category(impl_skill.get(\"category\", \"technical\")),\n",
    "                    level=SkillLevel.COMPETENT,\n",
    "                    context=SkillContext.HYBRID,\n",
    "                    confidence=impl_skill.get(\"confidence\", 0.6),\n",
    "                    source=f\"UNI:{course.code}_implicit\"\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            \n",
    "            # Analyze prerequisites (individual processing for now)\n",
    "            if course.prerequisites:\n",
    "                prereq_result = self.genai.analyze_prerequisites(course.prerequisites, texts[idx][:1000])\n",
    "                for prereq in prereq_result.get(\"prerequisites\", []):\n",
    "                    for impl_skill in prereq.get(\"implied_skills\", []):\n",
    "                        skill = Skill(\n",
    "                            name=impl_skill[\"name\"],\n",
    "                            category=SkillCategory.FOUNDATIONAL,\n",
    "                            level=SkillLevel.from_string(impl_skill.get(\"level\", \"competent\")),\n",
    "                            context=SkillContext.THEORETICAL,\n",
    "                            confidence=0.8,\n",
    "                            source=f\"UNI:{course.code}_prerequisite\"\n",
    "                        )\n",
    "                        skills.append(skill)\n",
    "            \n",
    "            # Apply context\n",
    "            context_result = context_results[idx] if idx < len(context_results) else {}\n",
    "            primary_context = context_result.get(\"context_analysis\", {}).get(\"primary_context\", \"hybrid\")\n",
    "            \n",
    "            for skill in skills:\n",
    "                if primary_context == \"practical\":\n",
    "                    skill.context = SkillContext.PRACTICAL\n",
    "                elif primary_context == \"theoretical\":\n",
    "                    skill.context = SkillContext.THEORETICAL\n",
    "                else:\n",
    "                    skill.context = SkillContext.HYBRID\n",
    "            \n",
    "            # Adjust skill levels based on study level\n",
    "            skills_dict = [{\"name\": s.name, \"level\": s.level.name} for s in skills]\n",
    "            if skills_dict:\n",
    "                level_adjustment = self.genai.adjust_skill_levels(skills_dict, course.study_level, texts[idx][:1000])\n",
    "                \n",
    "                for adj_skill in level_adjustment.get(\"adjusted_skills\", []):\n",
    "                    for skill in skills:\n",
    "                        if skill.name == adj_skill[\"skill_name\"]:\n",
    "                            skill.level = SkillLevel.from_string(adj_skill[\"adjusted_level\"])\n",
    "                            break\n",
    "            \n",
    "            # Deduplicate skills\n",
    "            skills = self._deduplicate_skills_with_ai(skills)\n",
    "            \n",
    "            # Cache and assign\n",
    "            cache_key = f\"uni_{course.code}\"\n",
    "            self.cache[cache_key] = skills\n",
    "            course.extracted_skills = skills\n",
    "            \n",
    "            logger.debug(f\"Extracted {len(skills)} skills from {course.code}\")\n",
    "    \n",
    "    def extract_from_vet_unit(self, unit: UnitOfCompetency) -> List[Skill]:\n",
    "        \"\"\"\n",
    "        Extract skills from a single VET unit (uses batch processing with size 1)\n",
    "        \n",
    "        Args:\n",
    "            unit: VET unit to extract skills from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted skills\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting skills from single VET unit: {unit.code}\")\n",
    "        \n",
    "        cache_key = f\"vet_{unit.code}\"\n",
    "        if cache_key in self.cache:\n",
    "            unit.extracted_skills = self.cache[cache_key]\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Process as a batch of 1\n",
    "        self._process_vet_batch([unit])\n",
    "        return unit.extracted_skills\n",
    "    \n",
    "    def extract_from_uni_course(self, course: UniCourse) -> List[Skill]:\n",
    "        \"\"\"\n",
    "        Extract skills from a single university course (uses batch processing with size 1)\n",
    "        \n",
    "        Args:\n",
    "            course: University course to extract skills from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted skills\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting skills from single Uni course: {course.code}\")\n",
    "        \n",
    "        cache_key = f\"uni_{course.code}\"\n",
    "        if cache_key in self.cache:\n",
    "            course.extracted_skills = self.cache[cache_key]\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Process as a batch of 1\n",
    "        self._process_uni_batch([course])\n",
    "        return course.extracted_skills\n",
    "    \n",
    "    def _validate_category(self, category_str: str) -> SkillCategory:\n",
    "        \"\"\"Validate and convert category string to SkillCategory enum\"\"\"\n",
    "        valid_categories = {\n",
    "            \"technical\": SkillCategory.TECHNICAL,\n",
    "            \"cognitive\": SkillCategory.COGNITIVE,\n",
    "            \"practical\": SkillCategory.PRACTICAL,\n",
    "            \"foundational\": SkillCategory.FOUNDATIONAL,\n",
    "            \"professional\": SkillCategory.PROFESSIONAL\n",
    "        }\n",
    "        \n",
    "        category_lower = category_str.lower().strip()\n",
    "        \n",
    "        if category_lower in valid_categories:\n",
    "            return valid_categories[category_lower]\n",
    "        \n",
    "        for valid_cat in valid_categories:\n",
    "            if valid_cat in category_lower or category_lower in valid_cat:\n",
    "                return valid_categories[valid_cat]\n",
    "        \n",
    "        logger.debug(f\"Invalid category '{category_str}' defaulting to 'technical'\")\n",
    "        return SkillCategory.TECHNICAL\n",
    "    \n",
    "    def _validate_context(self, context_str: str) -> SkillContext:\n",
    "        \"\"\"Validate and convert context string to SkillContext enum\"\"\"\n",
    "        valid_contexts = {\n",
    "            \"theoretical\": SkillContext.THEORETICAL,\n",
    "            \"practical\": SkillContext.PRACTICAL,\n",
    "            \"hybrid\": SkillContext.HYBRID\n",
    "        }\n",
    "        \n",
    "        context_lower = context_str.lower().strip()\n",
    "        \n",
    "        if context_lower in valid_contexts:\n",
    "            return valid_contexts[context_lower]\n",
    "        \n",
    "        logger.debug(f\"Invalid context '{context_str}' defaulting to 'hybrid'\")\n",
    "        return SkillContext.HYBRID\n",
    "    \n",
    "    def _convert_ai_skills(self, ai_skills: List[Dict], source: str) -> List[Skill]:\n",
    "        \"\"\"Convert AI-extracted skills to Skill objects\"\"\"\n",
    "        skills = []\n",
    "        \n",
    "        for ai_skill in ai_skills:\n",
    "            try:\n",
    "                skill_name = ai_skill.get(\"name\", \"\").strip()\n",
    "                if len(skill_name) < 3 or len(skill_name) > 100:\n",
    "                    continue\n",
    "                \n",
    "                category = self._validate_category(ai_skill.get(\"category\", \"technical\"))\n",
    "                context = self._validate_context(ai_skill.get(\"context\", \"hybrid\"))\n",
    "                \n",
    "                skill = Skill(\n",
    "                    name=skill_name,\n",
    "                    category=category,\n",
    "                    level=SkillLevel.from_string(ai_skill.get(\"level\", \"competent\")),\n",
    "                    context=context,\n",
    "                    keywords=ai_skill.get(\"keywords\", [])[:10],\n",
    "                    confidence=min(1.0, max(0.0, ai_skill.get(\"confidence\", 0.8))),\n",
    "                    source=source\n",
    "                )\n",
    "                skills.append(skill)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to convert AI skill: {e}\")\n",
    "        \n",
    "        return skills\n",
    "    \n",
    "    def _deduplicate_skills_with_ai(self, skills: List[Skill]) -> List[Skill]:\n",
    "        \"\"\"Deduplicate skills using Gen AI\"\"\"\n",
    "        if len(skills) <= 1:\n",
    "            return skills\n",
    "        \n",
    "        skills_dict = [\n",
    "            {\n",
    "                \"name\": s.name,\n",
    "                \"category\": s.category.value,\n",
    "                \"level\": s.level.name,\n",
    "                \"confidence\": s.confidence\n",
    "            }\n",
    "            for s in skills\n",
    "        ]\n",
    "        \n",
    "        dedup_result = self.genai.deduplicate_skills(skills_dict)\n",
    "        \n",
    "        final_skills = []\n",
    "        processed_names = set()\n",
    "        \n",
    "        for group in dedup_result.get(\"skill_groups\", []):\n",
    "            similar_names = [s.lower() for s in group.get(\"similar_skills\", [])]\n",
    "            merged_name = group.get(\"merged_name\", \"\")\n",
    "            \n",
    "            if merged_name:\n",
    "                best_skill = None\n",
    "                best_confidence = 0\n",
    "                \n",
    "                for skill in skills:\n",
    "                    if skill.name.lower() in similar_names:\n",
    "                        if skill.confidence > best_confidence:\n",
    "                            best_skill = skill\n",
    "                            best_confidence = skill.confidence\n",
    "                        processed_names.add(skill.name.lower())\n",
    "                \n",
    "                if best_skill:\n",
    "                    best_skill.name = merged_name\n",
    "                    merged_category = self._validate_category(group.get(\"merged_category\", best_skill.category.value))\n",
    "                    best_skill.category = merged_category\n",
    "                    best_skill.level = SkillLevel.from_string(group.get(\"merged_level\", best_skill.level.name))\n",
    "                    final_skills.append(best_skill)\n",
    "        \n",
    "        for skill_name in dedup_result.get(\"unique_skills\", []):\n",
    "            for skill in skills:\n",
    "                if skill.name == skill_name and skill.name.lower() not in processed_names:\n",
    "                    final_skills.append(skill)\n",
    "                    processed_names.add(skill.name.lower())\n",
    "        \n",
    "        for skill in skills:\n",
    "            if skill.name.lower() not in processed_names:\n",
    "                final_skills.append(skill)\n",
    "        \n",
    "        return final_skills\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the extraction cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        logger.info(\"Extraction cache cleared\")\n",
    "    \n",
    "    def get_extraction_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the extraction process\"\"\"\n",
    "        return {\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"extraction_mode\": \"vllm_batch_processing\",\n",
    "            \"features\": [\n",
    "                \"true_batch_processing\",\n",
    "                \"batch_skill_extraction\",\n",
    "                \"batch_context_determination\",\n",
    "                \"batch_study_level_detection\",\n",
    "                \"parallel_implicit_skill_detection\",\n",
    "                \"efficient_multi_unit_processing\",\n",
    "                \"optimized_for_qualifications\"\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47134c6-4377-410f-b94f-9ffdb82fd708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SkillMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "044f2f30-314e-4739-8b48-d44e5413eb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Skill mapping engine for credit transfer analysis using Gen AI\n",
    "\"\"\"\n",
    "\n",
    "# import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from models.base_models import Skill, SkillMapping\n",
    "from models.enums import SkillLevel, SkillContext, EMBEDDING_MODE\n",
    "# from interfaces.embedding_interface import EmbeddingInterface\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SkillMapper:\n",
    "    \"\"\"Maps skills between VET and University courses using Gen AI and embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embeddings: Optional[EmbeddingInterface] = None,\n",
    "                 genai: Optional[Any] = None,\n",
    "                 similarity_threshold: float = 0.8,\n",
    "                 partial_threshold: float = 0.6,\n",
    "                 embedding_mode: str = \"embedding\"):\n",
    "        \"\"\"\n",
    "        Initialize skill mapper with Gen AI support\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Embedding interface for similarity calculation\n",
    "            genai: GenAI interface for AI-based similarity\n",
    "            similarity_threshold: Threshold for direct skill matches\n",
    "            partial_threshold: Threshold for partial skill matches\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.genai = genai\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.partial_threshold = partial_threshold\n",
    "        self.similarity_cache = {}\n",
    "        self.embedding_mode = embedding_mode.lower()\n",
    "        if self.embedding_mode not in {\"hybrid\", \"genai\", \"embedding\"}:\n",
    "            logger.warning(f\"Unknown embedding mode '{self.embedding_mode}', defaulting to 'embedding'\")\n",
    "            self.embedding_mode = \"embedding\"\n",
    "    \n",
    "    def map_skills(self, \n",
    "                   vet_skills: List[Skill], \n",
    "                   uni_skills: List[Skill]) -> SkillMapping:\n",
    "        \"\"\"\n",
    "        Map VET skills to university skills using AI and embeddings\n",
    "        \n",
    "        Args:\n",
    "            vet_skills: List of VET skills\n",
    "            uni_skills: List of university skills\n",
    "            \n",
    "        Returns:\n",
    "            SkillMapping object with mapping details\n",
    "        \"\"\"\n",
    "        logger.info(f\"Mapping {len(vet_skills)} VET skills to {len(uni_skills)} Uni skills\")\n",
    "        \n",
    "        mapping = SkillMapping()\n",
    "        \n",
    "        if not vet_skills or not uni_skills:\n",
    "            logger.warning(\"Empty skill list provided for mapping\")\n",
    "            return mapping\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = self._calculate_similarity_matrix(vet_skills, uni_skills)\n",
    "        \n",
    "        # Find best matches\n",
    "        matched_vet = set()\n",
    "        matched_uni = set()\n",
    "        \n",
    "        # First pass: Find direct matches\n",
    "        for i, vet_skill in enumerate(vet_skills):\n",
    "            best_match_idx = np.argmax(similarity_matrix[i])\n",
    "            best_score = similarity_matrix[i][best_match_idx]\n",
    "            \n",
    "            if best_score >= self.similarity_threshold:\n",
    "                uni_skill = uni_skills[best_match_idx]\n",
    "                match_quality = self._assess_match_quality(vet_skill, uni_skill)\n",
    "                \n",
    "                mapping.direct_matches.append({\n",
    "                    \"vet_skill\": vet_skill,\n",
    "                    \"uni_skill\": uni_skill,\n",
    "                    \"similarity\": float(best_score),\n",
    "                    \"quality\": match_quality\n",
    "                })\n",
    "                matched_vet.add(i)\n",
    "                matched_uni.add(best_match_idx)\n",
    "                \n",
    "                # Mark this uni skill as matched\n",
    "                similarity_matrix[:, best_match_idx] = 0\n",
    "        \n",
    "        # Second pass: Find partial matches\n",
    "        for i, vet_skill in enumerate(vet_skills):\n",
    "            if i in matched_vet:\n",
    "                continue\n",
    "            \n",
    "            best_match_idx = np.argmax(similarity_matrix[i])\n",
    "            best_score = similarity_matrix[i][best_match_idx]\n",
    "            \n",
    "            if best_score >= self.partial_threshold:\n",
    "                uni_skill = uni_skills[best_match_idx]\n",
    "                match_quality = self._assess_match_quality(vet_skill, uni_skill)\n",
    "                gaps = self._identify_gaps(vet_skill, uni_skill)\n",
    "                \n",
    "                mapping.partial_matches.append({\n",
    "                    \"vet_skill\": vet_skill,\n",
    "                    \"uni_skill\": uni_skill,\n",
    "                    \"similarity\": float(best_score),\n",
    "                    \"quality\": match_quality,\n",
    "                    \"gaps\": gaps\n",
    "                })\n",
    "                matched_vet.add(i)\n",
    "                matched_uni.add(best_match_idx)\n",
    "        \n",
    "        # Identify unmapped skills\n",
    "        mapping.unmapped_vet = [\n",
    "            vet_skills[i] for i in range(len(vet_skills)) \n",
    "            if i not in matched_vet\n",
    "        ]\n",
    "        mapping.unmapped_uni = [\n",
    "            uni_skills[i] for i in range(len(uni_skills)) \n",
    "            if i not in matched_uni\n",
    "        ]\n",
    "        \n",
    "        # Calculate overall scores\n",
    "        mapping.coverage_score = self._calculate_coverage_score(mapping, uni_skills)\n",
    "        mapping.context_alignment = self._calculate_context_alignment(mapping)\n",
    "        \n",
    "        # Add metadata\n",
    "        mapping.metadata = {\n",
    "            \"total_vet_skills\": len(vet_skills),\n",
    "            \"total_uni_skills\": len(uni_skills),\n",
    "            \"direct_match_count\": len(mapping.direct_matches),\n",
    "            \"partial_match_count\": len(mapping.partial_matches),\n",
    "            \"unmapped_vet_count\": len(mapping.unmapped_vet),\n",
    "            \"unmapped_uni_count\": len(mapping.unmapped_uni),\n",
    "            \"use_ai\": self.genai is not None,\n",
    "            \"use_embeddings\": self.embeddings is not None\n",
    "        }\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Mapping complete: {len(mapping.direct_matches)} direct, \"\n",
    "            f\"{len(mapping.partial_matches)} partial, \"\n",
    "            f\"{len(mapping.unmapped_uni)} unmapped uni skills\"\n",
    "        )\n",
    "        \n",
    "        return mapping\n",
    "    \n",
    "    def _calculate_similarity_matrix(self, \n",
    "                                     vet_skills: List[Skill], \n",
    "                                     uni_skills: List[Skill]) -> np.ndarray:\n",
    "        \"\"\"Calculate pairwise similarity between skills using AI and/or embeddings\"\"\"\n",
    "        \n",
    "        # If both AI and embeddings available, use hybrid approach\n",
    "        if self.genai and self.embeddings and self.embedding_mode == EMBEDDING_MODE.HYBRID.value:\n",
    "            # Use embeddings for initial fast similarity\n",
    "            vet_names = [s.name for s in vet_skills]\n",
    "            uni_names = [s.name for s in uni_skills]\n",
    "            \n",
    "            vet_embeddings = self.embeddings.encode(vet_names)\n",
    "            uni_embeddings = self.embeddings.encode(uni_names)\n",
    "            \n",
    "            similarity_matrix = self.embeddings.similarity(vet_embeddings, uni_embeddings)\n",
    "            \n",
    "            # For high-scoring pairs, refine with AI\n",
    "            high_similarity_pairs = np.where(similarity_matrix > 0.5)\n",
    "            \n",
    "            for i, j in zip(high_similarity_pairs[0], high_similarity_pairs[1]):\n",
    "                # Use AI for more accurate similarity\n",
    "                ai_similarity = self.genai.analyze_skill_similarity(\n",
    "                    vet_skills[i].name,\n",
    "                    uni_skills[j].name\n",
    "                )\n",
    "                # Weighted average of embedding and AI similarity\n",
    "                similarity_matrix[i, j] = 0.6 * similarity_matrix[i, j] + 0.4 * ai_similarity\n",
    "        \n",
    "        # If only GenAI available, use it exclusively\n",
    "        elif self.genai and self.embedding_mode == EMBEDDING_MODE.GENAI.value:\n",
    "            similarity_matrix = np.zeros((len(vet_skills), len(uni_skills)))\n",
    "            \n",
    "            for i, vet_skill in enumerate(vet_skills):\n",
    "                for j, uni_skill in enumerate(uni_skills):\n",
    "                    # Check cache first\n",
    "                    cache_key = (vet_skill.name.lower(), uni_skill.name.lower())\n",
    "                    if cache_key in self.similarity_cache:\n",
    "                        similarity = self.similarity_cache[cache_key]\n",
    "                    else:\n",
    "                        similarity = self.genai.analyze_skill_similarity(\n",
    "                            vet_skill.name, \n",
    "                            uni_skill.name\n",
    "                        )\n",
    "                        self.similarity_cache[cache_key] = similarity\n",
    "                    \n",
    "                    similarity_matrix[i, j] = similarity\n",
    "        \n",
    "        # If only embeddings available, use them\n",
    "        elif self.embeddings and self.embedding_mode == EMBEDDING_MODE.EMBEDDING.value:\n",
    "            vet_names = [s.name for s in vet_skills]\n",
    "            uni_names = [s.name for s in uni_skills]\n",
    "            \n",
    "            vet_embeddings = self.embeddings.encode(vet_names)\n",
    "            uni_embeddings = self.embeddings.encode(uni_names)\n",
    "            \n",
    "            similarity_matrix = self.embeddings.similarity(vet_embeddings, uni_embeddings)\n",
    "        \n",
    "        # Fallback to simple string matching\n",
    "        else:\n",
    "            similarity_matrix = np.zeros((len(vet_skills), len(uni_skills)))\n",
    "            \n",
    "            for i, vet_skill in enumerate(vet_skills):\n",
    "                for j, uni_skill in enumerate(uni_skills):\n",
    "                    similarity = self._calculate_string_similarity(\n",
    "                        vet_skill.name, \n",
    "                        uni_skill.name\n",
    "                    )\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "        \n",
    "        # Apply modifiers based on skill properties\n",
    "        for i, vet_skill in enumerate(vet_skills):\n",
    "            for j, uni_skill in enumerate(uni_skills):\n",
    "                # Boost similarity for same category\n",
    "                if vet_skill.category == uni_skill.category:\n",
    "                    similarity_matrix[i, j] *= 1.1\n",
    "                \n",
    "                # Reduce similarity for very different contexts\n",
    "                if (vet_skill.context == SkillContext.PRACTICAL and \n",
    "                    uni_skill.context == SkillContext.THEORETICAL):\n",
    "                    similarity_matrix[i, j] *= 0.9\n",
    "                elif (vet_skill.context == SkillContext.THEORETICAL and \n",
    "                      uni_skill.context == SkillContext.PRACTICAL):\n",
    "                    similarity_matrix[i, j] *= 0.9\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        similarity_matrix = np.clip(similarity_matrix, 0, 1)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_string_similarity(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"Calculate string similarity as fallback\"\"\"\n",
    "        str1_lower = str1.lower().strip()\n",
    "        str2_lower = str2.lower().strip()\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = (str1_lower, str2_lower)\n",
    "        if cache_key in self.similarity_cache:\n",
    "            return self.similarity_cache[cache_key]\n",
    "        \n",
    "        # Exact match\n",
    "        if str1_lower == str2_lower:\n",
    "            similarity = 1.0\n",
    "        # One contains the other\n",
    "        elif str1_lower in str2_lower or str2_lower in str1_lower:\n",
    "            similarity = 0.85\n",
    "        else:\n",
    "            # Word overlap using Jaccard similarity\n",
    "            words1 = set(str1_lower.split())\n",
    "            words2 = set(str2_lower.split())\n",
    "            \n",
    "            if not words1 or not words2:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                intersection = words1.intersection(words2)\n",
    "                union = words1.union(words2)\n",
    "                similarity = len(intersection) / len(union) if union else 0.0\n",
    "        \n",
    "        # Cache result\n",
    "        self.similarity_cache[cache_key] = similarity\n",
    "        return similarity\n",
    "    \n",
    "    def _assess_match_quality(self, vet_skill: Skill, uni_skill: Skill) -> Dict[str, Any]:\n",
    "        \"\"\"Assess quality of skill match using AI if available\"\"\"\n",
    "        quality = {\n",
    "            \"level_alignment\": self._compare_levels(vet_skill.level, uni_skill.level),\n",
    "            \"context_compatibility\": self._compare_contexts(vet_skill.context, uni_skill.context),\n",
    "            \"category_match\": vet_skill.category == uni_skill.category,\n",
    "            \"confidence_product\": vet_skill.confidence * uni_skill.confidence\n",
    "        }\n",
    "        \n",
    "        # If AI available, get additional quality assessment\n",
    "        if self.genai:\n",
    "            # Could add a specific prompt for quality assessment\n",
    "            pass\n",
    "        \n",
    "        # Overall quality score (weighted average)\n",
    "        quality[\"overall\"] = (\n",
    "            quality[\"level_alignment\"] * 0.4 +\n",
    "            quality[\"context_compatibility\"] * 0.3 +\n",
    "            float(quality[\"category_match\"]) * 0.2 +\n",
    "            quality[\"confidence_product\"] * 0.1\n",
    "        )\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def _compare_levels(self, vet_level: SkillLevel, uni_level: SkillLevel) -> float:\n",
    "        \"\"\"Compare skill proficiency levels\"\"\"\n",
    "        try:\n",
    "            level_diff = vet_level.value - uni_level.value\n",
    "            \n",
    "            if level_diff >= 0:\n",
    "                # VET meets or exceeds requirement\n",
    "                return 1.0\n",
    "            elif level_diff == -1:\n",
    "                # One level below - minor gap\n",
    "                return 0.8\n",
    "            elif level_diff == -2:\n",
    "                # Two levels below - significant gap\n",
    "                return 0.6\n",
    "            else:\n",
    "                # Large gap\n",
    "                return 0.3\n",
    "        except:\n",
    "            # Fallback if comparison fails\n",
    "            return 0.5\n",
    "    \n",
    "    def _compare_contexts(self, vet_context: SkillContext, uni_context: SkillContext) -> float:\n",
    "        \"\"\"Compare skill contexts\"\"\"\n",
    "        if vet_context == uni_context:\n",
    "            return 1.0\n",
    "        elif vet_context == SkillContext.HYBRID or uni_context == SkillContext.HYBRID:\n",
    "            # Hybrid matches reasonably with both\n",
    "            return 0.85\n",
    "        else:\n",
    "            # Practical vs Theoretical mismatch\n",
    "            return 0.7\n",
    "    \n",
    "    def _identify_gaps(self, vet_skill: Skill, uni_skill: Skill) -> List[str]:\n",
    "        \"\"\"Identify gaps between VET and Uni skill requirements\"\"\"\n",
    "        gaps = []\n",
    "        \n",
    "        # Level gap\n",
    "        try:\n",
    "            if vet_skill.level.value < uni_skill.level.value:\n",
    "                level_diff = uni_skill.level.value - vet_skill.level.value\n",
    "                gaps.append(f\"Proficiency gap: {level_diff} level(s) below required {uni_skill.level.name}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Context gap\n",
    "        if vet_skill.context != uni_skill.context and uni_skill.context != SkillContext.HYBRID:\n",
    "            if uni_skill.context == SkillContext.THEORETICAL:\n",
    "                gaps.append(\"Context gap: requires more theoretical foundation\")\n",
    "            elif uni_skill.context == SkillContext.PRACTICAL:\n",
    "                gaps.append(\"Context gap: requires more practical application\")\n",
    "        \n",
    "        # Confidence gap\n",
    "        if vet_skill.confidence < 0.7:\n",
    "            gaps.append(\"Low confidence in skill extraction\")\n",
    "        \n",
    "        # Use AI for more detailed gap analysis if available\n",
    "        if self.genai:\n",
    "            # Could add specific gap analysis prompt\n",
    "            pass\n",
    "        \n",
    "        return gaps\n",
    "    \n",
    "    def _calculate_coverage_score(self, mapping: SkillMapping, uni_skills: List[Skill]) -> float:\n",
    "        \"\"\"Calculate skill coverage score\"\"\"\n",
    "        if not uni_skills:\n",
    "            return 0.0\n",
    "        \n",
    "        # Weight skills by importance\n",
    "        covered_count = len(mapping.direct_matches) + (len(mapping.partial_matches) * 0.5)\n",
    "        total_count = len(uni_skills)\n",
    "        \n",
    "        return min(1.0, covered_count / total_count) if total_count > 0 else 0.0\n",
    "    \n",
    "    def _calculate_context_alignment(self, mapping: SkillMapping) -> float:\n",
    "        \"\"\"Calculate overall context alignment score\"\"\"\n",
    "        if not mapping.direct_matches and not mapping.partial_matches:\n",
    "            return 0.0\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for match in mapping.direct_matches:\n",
    "            scores.append(match[\"quality\"][\"context_compatibility\"])\n",
    "        \n",
    "        for match in mapping.partial_matches:\n",
    "            scores.append(match[\"quality\"][\"context_compatibility\"] * 0.7)\n",
    "        \n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    def get_skill_alignment_summary(self, mapping: SkillMapping) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a summary of skill alignment\"\"\"\n",
    "        summary = {\n",
    "            \"total_alignment_score\": 0.0,\n",
    "            \"strengths\": [],\n",
    "            \"weaknesses\": [],\n",
    "            \"recommendations\": [],\n",
    "            \"analysis_method\": \"hybrid\" if self.genai and self.embeddings else \n",
    "                             \"ai_only\" if self.genai else \n",
    "                             \"embedding_only\" if self.embeddings else \"string_matching\"\n",
    "        }\n",
    "        \n",
    "        # Calculate total alignment score\n",
    "        summary[\"total_alignment_score\"] = (\n",
    "            mapping.coverage_score * 0.6 +\n",
    "            mapping.context_alignment * 0.4\n",
    "        )\n",
    "        \n",
    "        # Identify strengths\n",
    "        if mapping.coverage_score > 0.7:\n",
    "            summary[\"strengths\"].append(f\"Good skill coverage ({mapping.coverage_score:.1%})\")\n",
    "        if mapping.context_alignment > 0.8:\n",
    "            summary[\"strengths\"].append(\"Good practical/theoretical balance\")\n",
    "        \n",
    "        direct_ratio = len(mapping.direct_matches) / (len(mapping.direct_matches) + len(mapping.partial_matches)) if (mapping.direct_matches or mapping.partial_matches) else 0\n",
    "        if direct_ratio > 0.7:\n",
    "            summary[\"strengths\"].append(f\"High proportion of direct matches ({direct_ratio:.1%})\")\n",
    "        \n",
    "        # Identify weaknesses\n",
    "        if mapping.coverage_score < 0.5:\n",
    "            summary[\"weaknesses\"].append(f\"Low skill coverage ({mapping.coverage_score:.1%})\")\n",
    "        if mapping.context_alignment < 0.6:\n",
    "            summary[\"weaknesses\"].append(\"Poor practical/theoretical balance\")\n",
    "        if len(mapping.unmapped_uni) > 5:\n",
    "            summary[\"weaknesses\"].append(f\"{len(mapping.unmapped_uni)} critical skills not covered\")\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if mapping.unmapped_uni:\n",
    "            summary[\"recommendations\"].append(\n",
    "                f\"Bridge {len(mapping.unmapped_uni)} missing skills through supplementary training\"\n",
    "            )\n",
    "        \n",
    "        if mapping.context_alignment < 0.7:\n",
    "            if any(m[\"uni_skill\"].context == SkillContext.THEORETICAL for m in mapping.partial_matches):\n",
    "                summary[\"recommendations\"].append(\"Add theoretical foundation modules\")\n",
    "            if any(m[\"uni_skill\"].context == SkillContext.PRACTICAL for m in mapping.partial_matches):\n",
    "                summary[\"recommendations\"].append(\"Include more practical/lab work\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the similarity cache\"\"\"\n",
    "        self.similarity_cache.clear()\n",
    "        logger.info(\"Similarity cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "343a6970-2d8c-42e4-9f74-f30e7d8bb700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CreditTransferAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c6d4fb-372b-408e-9386-485b04151988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main credit transfer analyzer - Updated to support batch processing\n",
    "\"\"\"\n",
    "\n",
    "# import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "# from config import Config\n",
    "\n",
    "from models.base_models import (\n",
    "    VETQualification, UniQualification, CreditTransferRecommendation,\n",
    "    UnitOfCompetency, UniCourse, SkillMapping\n",
    ")\n",
    "from models.enums import RecommendationType\n",
    "# from extraction.skill_extractor import SkillExtractor\n",
    "# from mapping.skill_mapper import SkillMapper\n",
    "from mapping.edge_cases import EdgeCaseHandler\n",
    "from interfaces.genai_interface import GenAIInterface\n",
    "# from interfaces.embedding_interface import EmbeddingInterface\n",
    "\n",
    "# Import vLLM interfaces\n",
    "if Config.USE_VLLM:\n",
    "    if Config.USE_VLLM_BATCH:\n",
    "        pass\n",
    "        # from interfaces.vllm_genai_interface_batch import VLLMGenAIInterfaceBatch\n",
    "        # from extraction.vllm_skill_extractor_batch import VLLMSkillExtractorBatch\n",
    "    else:\n",
    "        from interfaces.vllm_genai_interface import VLLMGenAIInterface\n",
    "        from extraction.vllm_skill_extractor import VLLMSkillExtractor\n",
    "        from extraction.openai_skill_extractor import OpenAISkillExtractor\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CreditTransferAnalyzer:\n",
    "    \"\"\"Main analyzer for credit transfer recommendations with batch processing support\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 genai: Optional[GenAIInterface] = None,\n",
    "                 embeddings: Optional[EmbeddingInterface] = None,\n",
    "                 config: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize credit transfer analyzer with batch processing support\n",
    "        \n",
    "        Args:\n",
    "            genai: GenAI interface for advanced extraction\n",
    "            embeddings: Embedding interface for similarity\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.genai = genai\n",
    "        self.embeddings = embeddings\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # Initialize components based on interface type and configuration\n",
    "        if Config.USE_VLLM:\n",
    "            if Config.USE_VLLM_BATCH:\n",
    "                # Use batch processing versions\n",
    "                if isinstance(genai, VLLMGenAIInterfaceBatch):\n",
    "                    self.extractor = VLLMSkillExtractorBatch(genai, embeddings)\n",
    "                    logger.info(\"Using vLLM batch skill extractor for efficient processing\")\n",
    "                else:\n",
    "                    logger.warning(\"Batch mode configured but GenAI interface is not batch-enabled\")\n",
    "                    # Fall back to regular vLLM extractor\n",
    "                    from extraction.vllm_skill_extractor import VLLMSkillExtractor\n",
    "                    self.extractor = VLLMSkillExtractor(genai, embeddings)\n",
    "            else:\n",
    "                # Use regular vLLM versions\n",
    "                if hasattr(genai, '__class__') and 'VLLMGenAIInterface' in str(genai.__class__):\n",
    "                    self.extractor = VLLMSkillExtractor(genai, embeddings)\n",
    "                    logger.info(\"Using vLLM skill extractor (individual processing)\")\n",
    "                else:\n",
    "                    # Fall back to standard extractor\n",
    "                    self.extractor = SkillExtractor(genai, embeddings)\n",
    "                    \n",
    "        elif isinstance(genai, GenAIInterface) and hasattr(genai, 'client'):\n",
    "            # Use OpenAI extractor for Azure OpenAI\n",
    "            delay = self.config.get(\"openai_delay_between_requests\", 1.0)\n",
    "            \n",
    "            self.extractor = OpenAISkillExtractor(\n",
    "                genai, \n",
    "                embeddings, \n",
    "                delay_between_requests=delay\n",
    "            )\n",
    "            logger.info(\"Using OpenAI skill extractor\")\n",
    "            \n",
    "        else:\n",
    "            # Use standard extractor\n",
    "            self.extractor = SkillExtractor(genai, embeddings)\n",
    "            logger.info(\"Using standard skill extractor\")\n",
    "        \n",
    "        # Initialize mapper with Gen AI support\n",
    "        self.mapper = SkillMapper(embeddings, genai)\n",
    "        \n",
    "        # Initialize edge handler with Gen AI support\n",
    "        self.edge_handler = EdgeCaseHandler(genai)\n",
    "        \n",
    "        # Configuration\n",
    "        self.min_alignment_score = self.config.get(\"min_alignment_score\", 0.5)\n",
    "        self.combination_limit = self.config.get(\"max_unit_combination\", 3)\n",
    "        \n",
    "        # Analysis cache\n",
    "        self.analysis_cache = {}\n",
    "    \n",
    "    def analyze_transfer(self,\n",
    "                        vet_qual: VETQualification,\n",
    "                        uni_qual: UniQualification,\n",
    "                        target_courses: Optional[List[str]] = None) -> List[CreditTransferRecommendation]:\n",
    "        \"\"\"\n",
    "        Analyze credit transfer possibilities using batch processing if configured\n",
    "        \n",
    "        Args:\n",
    "            vet_qual: VET qualification to analyze\n",
    "            uni_qual: University qualification target\n",
    "            target_courses: Optional list of specific course codes to analyze\n",
    "            \n",
    "        Returns:\n",
    "            List of credit transfer recommendations\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting credit transfer analysis: {vet_qual.code} -> {uni_qual.code}\")\n",
    "        \n",
    "        # Check if batch extraction is available\n",
    "        use_batch = Config.USE_VLLM_BATCH and hasattr(self.extractor, 'extract_from_vet_qualification')\n",
    "        \n",
    "        if use_batch:\n",
    "            logger.info(\"Using batch processing for skill extraction\")\n",
    "            \n",
    "            # Extract all VET skills at once using batch processing\n",
    "            vet_skills_map = self.extractor.extract_from_vet_qualification(vet_qual)\n",
    "            logger.info(f\"Batch extracted skills from {len(vet_skills_map)} VET units\")\n",
    "            \n",
    "            # Filter target courses if specified\n",
    "            if target_courses:\n",
    "                # Create a temporary qualification with only target courses\n",
    "                filtered_qual = UniQualification(\n",
    "                    code=uni_qual.code,\n",
    "                    name=uni_qual.name,\n",
    "                    courses=[c for c in uni_qual.courses if c.code in target_courses]\n",
    "                )\n",
    "                # Extract all university skills at once using batch processing\n",
    "                uni_skills_map = self.extractor.extract_from_uni_qualification(filtered_qual)\n",
    "                courses_to_analyze = filtered_qual.courses\n",
    "            else:\n",
    "                # Extract all university skills at once using batch processing\n",
    "                uni_skills_map = self.extractor.extract_from_uni_qualification(uni_qual)\n",
    "                courses_to_analyze = uni_qual.courses\n",
    "            \n",
    "            logger.info(f\"Batch extracted skills from {len(uni_skills_map)} University courses\")\n",
    "            \n",
    "        else:\n",
    "            logger.info(\"Using individual processing for skill extraction\")\n",
    "            \n",
    "            # Extract skills from VET units individually\n",
    "            for unit in vet_qual.units:\n",
    "                if not unit.extracted_skills:\n",
    "                    self.extractor.extract_from_vet_unit(unit)\n",
    "            \n",
    "            # Filter target courses if specified\n",
    "            courses_to_analyze = uni_qual.courses\n",
    "            if target_courses:\n",
    "                courses_to_analyze = [c for c in uni_qual.courses if c.code in target_courses]\n",
    "            \n",
    "            # Extract skills from university courses individually\n",
    "            for course in courses_to_analyze:\n",
    "                if not course.extracted_skills:\n",
    "                    self.extractor.extract_from_uni_course(course)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Analyze each university course\n",
    "        for course in courses_to_analyze:\n",
    "            logger.info(f\"Analyzing transfers for {course.code}: {course.name}\")\n",
    "            \n",
    "            # Try single unit mappings\n",
    "            single_recs = self._analyze_single_mappings(vet_qual.units, course)\n",
    "            recommendations.extend(single_recs)\n",
    "            \n",
    "            # Try combination mappings if enabled\n",
    "            if self.combination_limit > 1:\n",
    "                combo_recs = self._analyze_combination_mappings(vet_qual.units, course)\n",
    "                recommendations.extend(combo_recs)\n",
    "        \n",
    "        # Filter and sort recommendations\n",
    "        recommendations = self._filter_recommendations(recommendations)\n",
    "        recommendations.sort(key=lambda x: (x.alignment_score, x.confidence), reverse=True)\n",
    "        \n",
    "        # Add analysis metadata\n",
    "        self._add_analysis_metadata(recommendations, vet_qual, uni_qual)\n",
    "        \n",
    "        logger.info(f\"Analysis complete: {len(recommendations)} recommendations generated\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _analyze_single_mappings(self,\n",
    "                                 units: List[UnitOfCompetency],\n",
    "                                 course: UniCourse) -> List[CreditTransferRecommendation]:\n",
    "        \"\"\"Analyze single unit to course mappings\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        for unit in units:\n",
    "            # Check cache\n",
    "            cache_key = f\"{unit.code}_{course.code}\"\n",
    "            if cache_key in self.analysis_cache:\n",
    "                rec = self.analysis_cache[cache_key]\n",
    "                if rec.alignment_score >= self.min_alignment_score:\n",
    "                    recommendations.append(rec)\n",
    "                continue\n",
    "            \n",
    "            # Perform analysis\n",
    "            rec = self._analyze_single_mapping(unit, course)\n",
    "            \n",
    "            # Cache result\n",
    "            self.analysis_cache[cache_key] = rec\n",
    "            \n",
    "            # Add if meets threshold\n",
    "            if rec.alignment_score >= self.min_alignment_score:\n",
    "                recommendations.append(rec)\n",
    "                logger.debug(f\"Single mapping {unit.code} -> {course.code}: {rec.alignment_score:.2%}\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _analyze_single_mapping(self,\n",
    "                               unit: UnitOfCompetency,\n",
    "                               course: UniCourse) -> CreditTransferRecommendation:\n",
    "        \"\"\"Analyze single unit to course mapping\"\"\"\n",
    "        \n",
    "        # Map skills\n",
    "        mapping = self.mapper.map_skills(unit.extracted_skills, course.extracted_skills)\n",
    "        \n",
    "        # Handle edge cases\n",
    "        edge_cases = self.edge_handler.process_edge_cases([unit], course, mapping)\n",
    "        \n",
    "        # Calculate scores\n",
    "        alignment_score = self._calculate_alignment_score(mapping, edge_cases)\n",
    "        skill_coverage = self._calculate_skill_coverage_breakdown(mapping)\n",
    "        confidence = self._calculate_confidence(mapping, edge_cases)\n",
    "        \n",
    "        # Determine recommendation type\n",
    "        recommendation_type = self._determine_recommendation_type(\n",
    "            alignment_score, mapping, edge_cases\n",
    "        )\n",
    "        \n",
    "        # Identify conditions\n",
    "        conditions = self._identify_conditions(mapping, edge_cases)\n",
    "        \n",
    "        # Generate evidence\n",
    "        evidence = self._generate_evidence(mapping, edge_cases)\n",
    "        \n",
    "        # Create recommendation\n",
    "        rec = CreditTransferRecommendation(\n",
    "            vet_units=[unit],\n",
    "            uni_course=course,\n",
    "            alignment_score=alignment_score,\n",
    "            skill_coverage=skill_coverage,\n",
    "            gaps=mapping.unmapped_uni,\n",
    "            evidence=evidence,\n",
    "            recommendation=recommendation_type,\n",
    "            conditions=conditions,\n",
    "            confidence=confidence,\n",
    "            edge_case_results=edge_cases\n",
    "        )\n",
    "        \n",
    "        return rec\n",
    "    \n",
    "    def _analyze_combination_mappings(self,\n",
    "                                      units: List[UnitOfCompetency],\n",
    "                                      course: UniCourse) -> List[CreditTransferRecommendation]:\n",
    "        \"\"\"Analyze combinations of units mapping to course\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        from itertools import combinations\n",
    "        \n",
    "        # Try combinations up to the limit\n",
    "        for r in range(2, min(self.combination_limit + 1, len(units) + 1)):\n",
    "            for combo in combinations(units, r):\n",
    "                # Skip if too many units\n",
    "                if len(combo) > self.combination_limit:\n",
    "                    continue\n",
    "                \n",
    "                # Combine skills from all units\n",
    "                combined_skills = []\n",
    "                for unit in combo:\n",
    "                    combined_skills.extend(unit.extracted_skills)\n",
    "                \n",
    "                # Map combined skills\n",
    "                mapping = self.mapper.map_skills(combined_skills, course.extracted_skills)\n",
    "                \n",
    "                # Only proceed if combination significantly improves coverage\n",
    "                if mapping.coverage_score < 0.7:\n",
    "                    continue\n",
    "                \n",
    "                # Perform full analysis\n",
    "                edge_cases = self.edge_handler.process_edge_cases(list(combo), course, mapping)\n",
    "                alignment_score = self._calculate_alignment_score(mapping, edge_cases)\n",
    "                \n",
    "                if alignment_score >= self.min_alignment_score * 1.2:  # Higher threshold for combinations\n",
    "                    rec = CreditTransferRecommendation(\n",
    "                        vet_units=list(combo),\n",
    "                        uni_course=course,\n",
    "                        alignment_score=alignment_score,\n",
    "                        skill_coverage=self._calculate_skill_coverage_breakdown(mapping),\n",
    "                        gaps=mapping.unmapped_uni,\n",
    "                        evidence=self._generate_evidence(mapping, edge_cases),\n",
    "                        recommendation=self._determine_recommendation_type(\n",
    "                            alignment_score, mapping, edge_cases\n",
    "                        ),\n",
    "                        conditions=self._identify_conditions(mapping, edge_cases),\n",
    "                        confidence=self._calculate_confidence(mapping, edge_cases),\n",
    "                        edge_case_results=edge_cases\n",
    "                    )\n",
    "                    recommendations.append(rec)\n",
    "                    \n",
    "                    logger.debug(\n",
    "                        f\"Combination {'+'.join(u.code for u in combo)} -> \"\n",
    "                        f\"{course.code}: {alignment_score:.2%}\"\n",
    "                    )\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _calculate_alignment_score(self,\n",
    "                                   mapping: SkillMapping,\n",
    "                                   edge_cases: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate overall alignment score\"\"\"\n",
    "        \n",
    "        # Base weights\n",
    "        weights = {\n",
    "            \"coverage\": 0.5,\n",
    "            \"context\": 0.25,\n",
    "            \"quality\": 0.15,\n",
    "            \"edge_penalty\": 0.1\n",
    "        }\n",
    "        \n",
    "        # Calculate quality score\n",
    "        quality_score = 0.0\n",
    "        if mapping.direct_matches:\n",
    "            quality_scores = [m[\"quality\"][\"overall\"] for m in mapping.direct_matches]\n",
    "            quality_score = sum(quality_scores) / len(quality_scores)\n",
    "        \n",
    "        # Calculate edge case penalty\n",
    "        edge_penalty = 0.0\n",
    "        \n",
    "        if \"context_imbalance\" in edge_cases:\n",
    "            edge_penalty += edge_cases[\"context_imbalance\"].get(\"imbalance_score\", 0) * 0.3\n",
    "        \n",
    "        if \"outdated_content\" in edge_cases:\n",
    "            if edge_cases[\"outdated_content\"].get(\"currency_issues\"):\n",
    "                edge_penalty += 0.2\n",
    "        \n",
    "        # Calculate final score\n",
    "        score = (\n",
    "            mapping.coverage_score * weights[\"coverage\"] +\n",
    "            mapping.context_alignment * weights[\"context\"] +\n",
    "            quality_score * weights[\"quality\"] -\n",
    "            edge_penalty * weights[\"edge_penalty\"]\n",
    "        )\n",
    "        \n",
    "        return max(0.0, min(1.0, score))\n",
    "    \n",
    "    def _determine_recommendation_type(self,\n",
    "                                       score: float,\n",
    "                                       mapping: SkillMapping,\n",
    "                                       edge_cases: Dict[str, Any]) -> RecommendationType:\n",
    "        \"\"\"Determine recommendation type based on analysis\"\"\"\n",
    "        \n",
    "        has_gaps = len(mapping.unmapped_uni) > 0\n",
    "        has_major_issues = any([\n",
    "            edge_cases.get(\"outdated_content\", {}).get(\"estimated_update_effort\") == \"high\",\n",
    "            edge_cases.get(\"context_imbalance\", {}).get(\"imbalance_score\", 0) > 0.5\n",
    "        ])\n",
    "        \n",
    "        if score >= 0.8 and not has_gaps and not has_major_issues:\n",
    "            return RecommendationType.FULL\n",
    "        elif score >= 0.7 and not has_major_issues:\n",
    "            return RecommendationType.CONDITIONAL\n",
    "        elif score >= 0.5:\n",
    "            return RecommendationType.PARTIAL\n",
    "        else:\n",
    "            return RecommendationType.NONE\n",
    "    \n",
    "    def _identify_conditions(self,\n",
    "                            mapping: SkillMapping,\n",
    "                            edge_cases: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Identify conditions for credit transfer\"\"\"\n",
    "        conditions = []\n",
    "        \n",
    "        # Missing skills\n",
    "        if mapping.unmapped_uni:\n",
    "            skill_names = [s.name for s in mapping.unmapped_uni[:3]]\n",
    "            conditions.append(f\"Bridge missing skills: {', '.join(skill_names)}\")\n",
    "        \n",
    "        # Context imbalance\n",
    "        if \"context_imbalance\" in edge_cases:\n",
    "            conditions.extend(edge_cases[\"context_imbalance\"].get(\"bridging_requirements\", []))\n",
    "        \n",
    "        # Outdated content\n",
    "        if \"outdated_content\" in edge_cases:\n",
    "            conditions.extend(edge_cases[\"outdated_content\"].get(\"update_requirements\", []))\n",
    "        \n",
    "        # Prerequisites\n",
    "        if \"prerequisite_chain\" in edge_cases:\n",
    "            missing = edge_cases[\"prerequisite_chain\"].get(\"missing_prerequisites\", [])\n",
    "            if missing:\n",
    "                conditions.append(f\"Complete prerequisites: {', '.join(missing[:3])}\")\n",
    "        \n",
    "        return conditions\n",
    "    \n",
    "    def _generate_evidence(self,\n",
    "                          mapping: SkillMapping,\n",
    "                          edge_cases: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate evidence statements for recommendation\"\"\"\n",
    "        evidence = []\n",
    "        \n",
    "        # Skill matches\n",
    "        if mapping.direct_matches:\n",
    "            evidence.append(f\"{len(mapping.direct_matches)} direct skill matches\")\n",
    "        \n",
    "        if mapping.partial_matches:\n",
    "            evidence.append(f\"{len(mapping.partial_matches)} partial skill matches\")\n",
    "        \n",
    "        # Coverage metrics\n",
    "        evidence.append(f\"Skill coverage: {mapping.coverage_score:.1%}\")\n",
    "        \n",
    "        if mapping.context_alignment > 0.8:\n",
    "            evidence.append(\"Good practical/theoretical balance\")\n",
    "        \n",
    "        # Edge case findings\n",
    "        if \"split_to_single\" in edge_cases:\n",
    "            if edge_cases[\"split_to_single\"].get(\"total_coverage\", 0) > 0.8:\n",
    "                evidence.append(\"Unit combination provides comprehensive coverage\")\n",
    "        \n",
    "        if \"credit_hours\" in edge_cases:\n",
    "            if not edge_cases[\"credit_hours\"].get(\"adjustment_needed\", False):\n",
    "                evidence.append(\"Credit hour alignment acceptable\")\n",
    "        \n",
    "        # Add processing method\n",
    "        if Config.USE_VLLM_BATCH:\n",
    "            evidence.append(\"Analyzed using batch-processed skill extraction\")\n",
    "        elif mapping.metadata.get(\"use_ai\"):\n",
    "            evidence.append(\"Analyzed using AI-powered skill matching\")\n",
    "        \n",
    "        return evidence\n",
    "    \n",
    "    def _calculate_skill_coverage_breakdown(self,\n",
    "                                           mapping: SkillMapping) -> Dict[str, float]:\n",
    "        \"\"\"Calculate detailed skill coverage breakdown\"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        breakdown = defaultdict(float)\n",
    "        category_totals = defaultdict(int)\n",
    "        category_matched = defaultdict(float)\n",
    "        \n",
    "        # Count totals by category\n",
    "        for match in mapping.direct_matches:\n",
    "            category = match[\"uni_skill\"].category.value\n",
    "            category_matched[category] += 1.0\n",
    "            category_totals[category] += 1\n",
    "        \n",
    "        for match in mapping.partial_matches:\n",
    "            category = match[\"uni_skill\"].category.value\n",
    "            category_matched[category] += 0.5\n",
    "            category_totals[category] += 1\n",
    "        \n",
    "        for skill in mapping.unmapped_uni:\n",
    "            category = skill.category.value\n",
    "            category_totals[category] += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        for category in category_totals:\n",
    "            if category_totals[category] > 0:\n",
    "                breakdown[category] = category_matched[category] / category_totals[category]\n",
    "        \n",
    "        return dict(breakdown)\n",
    "    \n",
    "    def _calculate_confidence(self,\n",
    "                             mapping: SkillMapping,\n",
    "                             edge_cases: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate confidence in the recommendation\"\"\"\n",
    "        confidence = 1.0\n",
    "        \n",
    "        # Reduce for unmapped skills\n",
    "        unmapped_ratio = len(mapping.unmapped_uni) / (\n",
    "            len(mapping.direct_matches) + len(mapping.partial_matches) + len(mapping.unmapped_uni)\n",
    "        ) if (mapping.direct_matches or mapping.partial_matches or mapping.unmapped_uni) else 0\n",
    "        \n",
    "        confidence -= unmapped_ratio * 0.3\n",
    "        \n",
    "        # Reduce for edge case issues\n",
    "        if \"context_imbalance\" in edge_cases:\n",
    "            imbalance = edge_cases[\"context_imbalance\"].get(\"imbalance_score\", 0)\n",
    "            confidence -= imbalance * 0.2\n",
    "        \n",
    "        if \"outdated_content\" in edge_cases:\n",
    "            if edge_cases[\"outdated_content\"].get(\"currency_issues\"):\n",
    "                confidence -= 0.15\n",
    "        \n",
    "        # Boost for strong direct matches\n",
    "        if mapping.direct_matches:\n",
    "            direct_ratio = len(mapping.direct_matches) / (\n",
    "                len(mapping.direct_matches) + len(mapping.partial_matches)\n",
    "            ) if (mapping.direct_matches or mapping.partial_matches) else 0\n",
    "            \n",
    "            if direct_ratio > 0.7:\n",
    "                confidence += 0.1\n",
    "        \n",
    "        # Boost if using AI or batch processing\n",
    "        if Config.USE_VLLM_BATCH:\n",
    "            confidence += 0.07  # Higher boost for batch processing\n",
    "        elif mapping.metadata.get(\"use_ai\"):\n",
    "            confidence += 0.05\n",
    "        \n",
    "        return max(0.3, min(1.0, confidence))\n",
    "    \n",
    "    def _filter_recommendations(self,\n",
    "                               recommendations: List[CreditTransferRecommendation]\n",
    "                               ) -> List[CreditTransferRecommendation]:\n",
    "        \"\"\"Filter recommendations to avoid duplicates and conflicts\"\"\"\n",
    "        filtered = []\n",
    "        seen_mappings = set()\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            # Create unique key for this mapping\n",
    "            vet_codes = tuple(sorted(rec.get_vet_unit_codes()))\n",
    "            uni_code = rec.uni_course.code\n",
    "            mapping_key = (vet_codes, uni_code)\n",
    "            \n",
    "            # Skip if already seen\n",
    "            if mapping_key in seen_mappings:\n",
    "                continue\n",
    "            \n",
    "            seen_mappings.add(mapping_key)\n",
    "            \n",
    "            # Skip if recommendation is \"none\"\n",
    "            if rec.recommendation == RecommendationType.NONE:\n",
    "                continue\n",
    "            \n",
    "            filtered.append(rec)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _add_analysis_metadata(self,\n",
    "                               recommendations: List[CreditTransferRecommendation],\n",
    "                               vet_qual: VETQualification,\n",
    "                               uni_qual: UniQualification):\n",
    "        \"\"\"Add metadata to recommendations\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            rec.metadata.update({\n",
    "                \"vet_qualification\": vet_qual.code,\n",
    "                \"uni_qualification\": uni_qual.code,\n",
    "                \"analysis_timestamp\": timestamp,\n",
    "                \"analyzer_version\": \"2.1.0\",  # Updated version for batch support\n",
    "                \"extractor_type\": type(self.extractor).__name__,\n",
    "                \"uses_genai\": self.genai is not None,\n",
    "                \"uses_embeddings\": self.embeddings is not None,\n",
    "                \"uses_batch_processing\": Config.USE_VLLM_BATCH,\n",
    "                \"batch_size\": Config.VLLM_BATCH_SIZE if Config.USE_VLLM_BATCH else 1\n",
    "            })\n",
    "    \n",
    "    def export_recommendations(self,\n",
    "                               recommendations: List[CreditTransferRecommendation],\n",
    "                               filepath: str):\n",
    "        \"\"\"Export recommendations to JSON file\"\"\"\n",
    "        data = {\n",
    "            \"recommendations\": [rec.to_dict() for rec in recommendations],\n",
    "            \"summary\": self._generate_summary(recommendations),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"extractor_stats\": self._get_extractor_stats(),\n",
    "            \"analysis_method\": \"batch_processing\" if Config.USE_VLLM_BATCH else \"individual_processing\",\n",
    "            \"configuration\": {\n",
    "                \"use_batch\": Config.USE_VLLM_BATCH,\n",
    "                \"batch_size\": Config.VLLM_BATCH_SIZE if Config.USE_VLLM_BATCH else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Exported {len(recommendations)} recommendations to {filepath}\")\n",
    "    \n",
    "    def _generate_summary(self,\n",
    "                         recommendations: List[CreditTransferRecommendation]) -> Dict:\n",
    "        \"\"\"Generate summary of recommendations\"\"\"\n",
    "        full = [r for r in recommendations if r.recommendation == RecommendationType.FULL]\n",
    "        conditional = [r for r in recommendations if r.recommendation == RecommendationType.CONDITIONAL]\n",
    "        partial = [r for r in recommendations if r.recommendation == RecommendationType.PARTIAL]\n",
    "        \n",
    "        return {\n",
    "            \"total_recommendations\": len(recommendations),\n",
    "            \"full_credit\": len(full),\n",
    "            \"conditional_credit\": len(conditional),\n",
    "            \"partial_credit\": len(partial),\n",
    "            \"average_alignment\": sum(r.alignment_score for r in recommendations) / len(recommendations) if recommendations else 0,\n",
    "            \"average_confidence\": sum(r.confidence for r in recommendations) / len(recommendations) if recommendations else 0,\n",
    "            \"uses_ai\": self.genai is not None,\n",
    "            \"uses_batch_processing\": Config.USE_VLLM_BATCH\n",
    "        }\n",
    "    \n",
    "    def _get_extractor_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics from the extractor\"\"\"\n",
    "        if hasattr(self.extractor, 'get_extraction_stats'):\n",
    "            return self.extractor.get_extraction_stats()\n",
    "        elif hasattr(self.extractor, 'cache'):\n",
    "            return {\"cache_size\": len(self.extractor.cache)}\n",
    "        else:\n",
    "            return {\"extractor_type\": type(self.extractor).__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34272a15-33b0-45a1-8bd7-3bb9fa35b0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c2faf1-390e-4eac-83c7-1f6e2cb945f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "# import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "# from config import Config\n",
    "from models.base_models import VETQualification, UniQualification, UnitOfCompetency, UniCourse\n",
    "from interfaces.genai_interface import GenAIInterface\n",
    "from interfaces.vllm_genai_interface import VLLMGenAIInterface\n",
    "# from interfaces.embedding_interface import EmbeddingInterface\n",
    "# from analysis.analyzer import CreditTransferAnalyzer\n",
    "from reporting.report_generator import ReportGenerator\n",
    "\n",
    "def load_vet_data(filepath: str) -> VETQualification:\n",
    "    \"\"\"Load VET qualification data from JSON file\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    vet_qual = VETQualification(\n",
    "        code=data[\"code\"],\n",
    "        name=data[\"name\"],\n",
    "        level=data[\"level\"]\n",
    "    )\n",
    "    \n",
    "    for unit_data in data.get(\"units\", []):\n",
    "        unit = UnitOfCompetency(\n",
    "            code=unit_data[\"code\"],\n",
    "            name=unit_data[\"name\"],\n",
    "            description=unit_data.get(\"description\", \"\"),\n",
    "            learning_outcomes=unit_data.get(\"learning_outcomes\", []),\n",
    "            assessment_requirements=unit_data.get(\"assessment_requirements\", \"\"),\n",
    "            nominal_hours=unit_data.get(\"nominal_hours\", 0),\n",
    "            prerequisites=unit_data.get(\"prerequisites\", [])\n",
    "        )\n",
    "        vet_qual.units.append(unit)\n",
    "    \n",
    "    logger.info(f\"Loaded VET qualification: {vet_qual.code} with {len(vet_qual.units)} units\")\n",
    "    return vet_qual\n",
    "\n",
    "\n",
    "def load_uni_data(filepath: str) -> UniQualification:\n",
    "    \"\"\"Load university qualification data from JSON file\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    uni_qual = UniQualification(\n",
    "        code=data[\"code\"],\n",
    "        name=data[\"name\"]\n",
    "    )\n",
    "    \n",
    "    for course_data in data.get(\"courses\", []):\n",
    "        course = UniCourse(\n",
    "            code=course_data[\"code\"],\n",
    "            name=course_data[\"name\"],\n",
    "            description=course_data.get(\"description\", \"\"),\n",
    "            study_level=course_data.get(\"study_level\", \"intermediate\"),\n",
    "            learning_outcomes=course_data.get(\"learning_outcomes\", []),\n",
    "            prerequisites=course_data.get(\"prerequisites\", []),\n",
    "            credit_points=course_data.get(\"credit_points\", 0),\n",
    "            topics=course_data.get(\"topics\", []),\n",
    "            assessment=course_data.get(\"assessment\", \"\")\n",
    "        )\n",
    "        uni_qual.courses.append(course)\n",
    "    \n",
    "    logger.info(f\"Loaded university qualification: {uni_qual.code} with {len(uni_qual.courses)} courses\")\n",
    "    return uni_qual\n",
    "\n",
    "\n",
    "def initialize_interfaces():\n",
    "    \"\"\"Initialize GenAI and Embedding interfaces with batch processing support\"\"\"\n",
    "    genai = None\n",
    "    embeddings = None\n",
    "\n",
    "    # Save the original CUDA_VISIBLE_DEVICES\n",
    "    original_cuda_visible = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "    logger.info(f\"original_cuda_visible : {original_cuda_visible}\")\n",
    "    # Initialize GenAI - Priority order: Azure OpenAI > vLLM > Web API\n",
    "    if Config.USE_AZURE_OPENAI:\n",
    "        try:\n",
    "            azure_config = Config.get_azure_openai_config()\n",
    "            genai = GenAIInterface(\n",
    "                endpoint=azure_config[\"endpoint\"],\n",
    "                deployment=azure_config[\"deployment\"],\n",
    "                api_key=azure_config[\"api_key\"],\n",
    "                api_version=azure_config[\"api_version\"],\n",
    "                timeout=azure_config[\"timeout\"],\n",
    "                max_tokens=azure_config[\"max_tokens\"],\n",
    "                temperature=azure_config[\"temperature\"]\n",
    "            )\n",
    "            logger.info(f\"Azure OpenAI interface initialized with deployment: {azure_config['deployment']}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Azure OpenAI interface: {e}\")\n",
    "            \n",
    "            # Fall back to vLLM if Azure OpenAI fails\n",
    "            if Config.USE_VLLM:\n",
    "                try:\n",
    "                    if Config.USE_VLLM_BATCH:\n",
    "                        genai = VLLMGenAIInterfaceBatch(\n",
    "                            model_name=Config.VLLM_MODEL_NAME,\n",
    "                            number_gpus=Config.VLLM_NUM_GPUS,\n",
    "                            max_model_len=Config.VLLM_MAX_MODEL_LEN,\n",
    "                            batch_size=Config.VLLM_BATCH_SIZE,\n",
    "                            model_cache_dir=Config.MODEL_CACHE_DIR,\n",
    "                            external_model_dir=Config.EXTERNAL_MODEL_DIR,\n",
    "                            gpu_id=0  # Explicitly use GPU 0\n",
    "                        )\n",
    "                        logger.info(f\"Fell back to vLLM batch interface on GPU 0 with model: {Config.VLLM_MODEL_NAME}\")\n",
    "                    else:\n",
    "                        genai = VLLMGenAIInterface(\n",
    "                            model_name=Config.VLLM_MODEL_NAME,\n",
    "                            number_gpus=Config.VLLM_NUM_GPUS,\n",
    "                            max_model_len=Config.VLLM_MAX_MODEL_LEN,\n",
    "                            model_cache_dir=Config.MODEL_CACHE_DIR,\n",
    "                            external_model_dir=Config.EXTERNAL_MODEL_DIR,\n",
    "                            gpu_id=0  # Explicitly use GPU 0\n",
    "                        )\n",
    "                        logger.info(f\"Fell back to vLLM interface on GPU 0 with model: {Config.VLLM_MODEL_NAME}\")\n",
    "                except Exception as e2:\n",
    "                    logger.warning(f\"Failed to initialize vLLM interface: {e2}\")\n",
    "    \n",
    "    elif Config.USE_VLLM:\n",
    "        try:\n",
    "            # Set CUDA_VISIBLE_DEVICES for vLLM (GPU 0)\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "            \n",
    "            if Config.USE_VLLM_BATCH:\n",
    "                genai = VLLMGenAIInterfaceBatch(\n",
    "                    model_name=Config.VLLM_MODEL_NAME,\n",
    "                    number_gpus=Config.VLLM_NUM_GPUS,\n",
    "                    max_model_len=Config.VLLM_MAX_MODEL_LEN,\n",
    "                    batch_size=Config.VLLM_BATCH_SIZE,\n",
    "                    model_cache_dir=Config.MODEL_CACHE_DIR,\n",
    "                    external_model_dir=Config.EXTERNAL_MODEL_DIR,\n",
    "                    gpu_id=0  # Using GPU 0\n",
    "                )\n",
    "                logger.info(f\"vLLM batch GenAI interface initialized on GPU 0 with model: {Config.VLLM_MODEL_NAME}\")\n",
    "                logger.info(f\"Batch size: {Config.VLLM_BATCH_SIZE}\")\n",
    "            else:\n",
    "                genai = VLLMGenAIInterface(\n",
    "                    model_name=Config.VLLM_MODEL_NAME,\n",
    "                    number_gpus=Config.VLLM_NUM_GPUS,\n",
    "                    max_model_len=Config.VLLM_MAX_MODEL_LEN,\n",
    "                    model_cache_dir=Config.MODEL_CACHE_DIR,\n",
    "                    external_model_dir=Config.EXTERNAL_MODEL_DIR,\n",
    "                    gpu_id=0  # Using GPU 0\n",
    "                )\n",
    "                logger.info(f\"vLLM GenAI interface initialized on GPU 0 with model: {Config.VLLM_MODEL_NAME}\")\n",
    "                \n",
    "            # Restore original CUDA_VISIBLE_DEVICES for embedding model\n",
    "            if original_cuda_visible is not None:\n",
    "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = original_cuda_visible\n",
    "            else:\n",
    "                # Remove the variable to make all GPUs visible again\n",
    "                del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize vLLM GenAI interface: {e}\")\n",
    "            \n",
    "            # Fall back to web API if vLLM fails\n",
    "            if Config.USE_GENAI:\n",
    "                try:\n",
    "                    genai = GenAIInterface(\n",
    "                        model_endpoint=Config.GENAI_ENDPOINT,\n",
    "                        api_key=Config.GENAI_API_KEY,\n",
    "                        timeout=Config.GENAI_TIMEOUT\n",
    "                    )\n",
    "                    logger.info(\"Fell back to web API GenAI interface\")\n",
    "                except Exception as e2:\n",
    "                    logger.warning(f\"Failed to initialize web API GenAI interface: {e2}\")\n",
    "    \n",
    "    elif Config.USE_GENAI:\n",
    "        try:\n",
    "            genai = GenAIInterface(\n",
    "                model_endpoint=Config.GENAI_ENDPOINT,\n",
    "                api_key=Config.GENAI_API_KEY,\n",
    "                timeout=Config.GENAI_TIMEOUT\n",
    "            )\n",
    "            logger.info(\"Web API GenAI interface initialized\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize GenAI interface: {e}\")\n",
    "    \n",
    "    # Initialize Embeddings\n",
    "    try:\n",
    "        # Use cuda:1 for embeddings when using vLLM on cuda:0\n",
    "        embedding_device = \"cuda:1\" if Config.USE_VLLM else Config.EMBEDDING_DEVICE\n",
    "        \n",
    "        embeddings = EmbeddingInterface(\n",
    "            model_name=Config.EMBEDDING_MODEL_NAME,\n",
    "            model_cache_dir=Config.MODEL_CACHE_DIR,\n",
    "            external_model_dir=Config.EXTERNAL_MODEL_DIR,\n",
    "            device=embedding_device,  # This will be cuda:1 when using vLLM\n",
    "            batch_size=Config.EMBEDDING_BATCH_SIZE\n",
    "        )\n",
    "        logger.info(f\"Embedding interface initialized with model: {Config.EMBEDDING_MODEL_NAME} on device: {embedding_device}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to initialize Embedding interface: {e}\")\n",
    "        # Try fallback to legacy configuration\n",
    "        try:\n",
    "            embeddings = EmbeddingInterface(\n",
    "                model_name=Config.EMBEDDING_MODEL_NAME,\n",
    "                model_cache_dir=Config.MODEL_CACHE_DIR,\n",
    "                external_model_dir=Config.EXTERNAL_MODEL_DIR,\n",
    "                device=embedding_device,  # This will be cuda:1 when using vLLM\n",
    "                batch_size=Config.EMBEDDING_BATCH_SIZE\n",
    "            )\n",
    "            logger.info(\"Initialized embedding interface with legacy configuration\")\n",
    "        except Exception as e2:\n",
    "            logger.warning(f\"Failed to initialize legacy embedding interface: {e2}\")\n",
    "    \n",
    "    return genai, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa70ff5-30a0-4cb3-9c95-6d91d3a02ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Starting credit transfer analysis\")\n",
    "vet_file = \"./data/sample_vet.json\"\n",
    "uni_file = \"./data/sample_uni.json\"   \n",
    "    # Load data\n",
    "vet_qual = load_vet_data(vet_file)\n",
    "uni_qual = load_uni_data(uni_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f39f35-4295-4017-99a5-3f71eb0dfd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "genai, embeddings = initialize_interfaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44a4786-98b1-4b45-af0a-abcafdd8e684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "analyzer = CreditTransferAnalyzer(\n",
    "        genai=genai,\n",
    "        embeddings=embeddings,\n",
    "        config=Config.get_config_dict()\n",
    "    )\n",
    "    \n",
    "# Perform analysis\n",
    "logger.info(\"Performing credit transfer analysis...\")\n",
    "recommendations = analyzer.analyze_transfer(\n",
    "    vet_qual=vet_qual,\n",
    "    uni_qual=uni_qual,\n",
    "    target_courses=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac481efc-6aa8-4d00-a7ae-81ae43887276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Generated {len(recommendations)} recommendations\")\n",
    "    \n",
    "    # Generate report\n",
    "report_gen = ReportGenerator()\n",
    "  \n",
    "# Export skills if requested\n",
    "logger.info(\"Exporting extracted skills...\")\n",
    "\n",
    "# Generate complete report package including skills\n",
    "files = report_gen.generate_complete_report_package(\n",
    "    recommendations, vet_qual, uni_qual\n",
    ")\n",
    "\n",
    "logger.info(\"Report package generated:\")\n",
    "for file_type, filepath in files.items():\n",
    "    logger.info(f\"  {file_type}: {filepath}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REPORT PACKAGE GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Processing Mode: {'Batch' if Config.USE_VLLM_BATCH else 'Individual'}\")\n",
    "if Config.USE_VLLM_BATCH:\n",
    "    print(f\"Batch Size: {Config.VLLM_BATCH_SIZE}\")\n",
    "print(f\"Main report: {files.get('report_html', 'N/A')}\")\n",
    "print(f\"Recommendations: {files.get('recommendations_json', 'N/A')}\")\n",
    "print(f\"VET Skills: {files.get('vet_skills_json', 'N/A')}\")\n",
    "print(f\"University Skills: {files.get('uni_skills_json', 'N/A')}\")\n",
    "print(f\"Combined Skills: {files.get('combined_skills_json', 'N/A')}\")\n",
    "print(f\"Skill Analysis: {files.get('skill_analysis', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c07a7643-307d-433d-9fb7-7cb5ad9a72c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
